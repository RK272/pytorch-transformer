{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 2\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Why.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(3, input_size)\n",
    "targets = torch.tensor([1], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0156],\n",
       "        [-0.2300],\n",
       "        [ 0.3495]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.zeros(hidden_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([ 0.6726,  1.2816,  1.1699,  0.0662, -0.4349, -1.1138,  0.2697,  0.8744,\n",
      "        -0.7502,  0.6999])\n",
      "*********\n",
      "input tensor([-0.7518, -2.6000, -0.2836,  0.2714, -0.5050,  1.4682,  0.7225,  1.6955,\n",
      "        -2.0130,  1.1156])\n",
      "*********\n",
      "input tensor([-0.1412,  0.0848,  1.2653,  1.0463,  2.4844,  0.3462,  0.4163,  0.2199,\n",
      "        -1.4097, -1.2408])\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "for i in range(inputs.size(0)):\n",
    "    input = inputs[i]\n",
    "    print(\"input\", input)\n",
    "    print(\"*********\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} tensor([ 1.0000,  1.0000,  0.9992,  0.9986,  0.8267, -0.9668, -1.0000, -1.0000,\n",
      "         1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000,  0.9997,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  0.7643,\n",
      "         1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  0.8599,  1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.9978,  1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         0.9999,  0.9998, -1.0000, -1.0000, -1.0000, -1.0000,  0.6859, -1.0000,\n",
      "         0.9998, -0.9745, -1.0000, -1.0000,  0.9881, -1.0000,  1.0000,  1.0000,\n",
      "        -1.0000,  1.0000, -0.9987,  1.0000,  1.0000,  1.0000,  1.0000,  0.9881,\n",
      "         1.0000, -1.0000, -0.6342, -1.0000,  1.0000, -0.9999, -1.0000,  1.0000,\n",
      "         1.0000, -0.9995,  1.0000, -0.9473, -0.6501, -1.0000,  0.9368, -0.9992,\n",
      "        -0.9998,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -0.9984,\n",
      "        -1.0000,  0.6631, -0.9976, -0.8585, -1.0000, -1.0000,  1.0000,  0.3656,\n",
      "        -0.9570, -1.0000,  1.0000,  1.0000,  0.2010, -0.9956, -1.0000,  1.0000,\n",
      "        -1.0000,  1.0000, -1.0000, -0.9637, -0.8464, -0.9984, -1.0000, -0.7103,\n",
      "         0.9961, -1.0000,  0.9851, -1.0000, -1.0000, -0.8710,  0.3442,  0.9961,\n",
      "         1.0000, -0.9936,  1.0000,  0.9996,  1.0000, -1.0000,  0.3965,  1.0000],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "{1} tensor([ 1.0000, -1.0000, -1.0000, -0.8006,  1.0000,  1.0000,  1.0000, -0.9996,\n",
      "        -1.0000, -0.9564, -1.0000,  1.0000, -1.0000, -1.0000,  0.9989, -0.0252,\n",
      "         1.0000,  0.9999,  1.0000, -1.0000, -0.9968,  0.8563,  1.0000, -1.0000,\n",
      "         1.0000,  0.1856,  1.0000, -1.0000,  0.9781,  1.0000,  0.9778, -1.0000,\n",
      "         0.9969,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  0.9974,\n",
      "         0.9939,  0.2235, -1.0000,  1.0000, -0.9295, -1.0000,  1.0000,  1.0000,\n",
      "         0.9826, -1.0000, -1.0000, -1.0000,  0.9996,  0.9065, -1.0000,  0.9998,\n",
      "        -1.0000,  0.5541,  1.0000, -1.0000, -0.7071,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000, -0.9181,  0.9992,  1.0000,  1.0000,  0.5344, -0.6706,  1.0000,\n",
      "         0.9871, -1.0000, -1.0000, -1.0000, -1.0000,  0.8157,  1.0000,  0.9990,\n",
      "        -1.0000, -1.0000, -1.0000, -0.9994,  0.9995,  1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -0.7670, -1.0000, -0.9869,  1.0000,  1.0000, -1.0000,\n",
      "        -1.0000,  0.0073,  1.0000,  1.0000,  1.0000,  0.9999, -1.0000,  1.0000,\n",
      "        -1.0000,  0.0251,  0.9989, -1.0000,  1.0000,  1.0000,  0.9820,  1.0000,\n",
      "        -1.0000, -1.0000,  0.9988, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "{2} tensor([ 1.0000,  0.4069,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000, -0.9848,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -0.7183,\n",
      "        -0.5534,  1.0000,  1.0000, -1.0000,  1.0000, -0.8777, -1.0000, -1.0000,\n",
      "         1.0000,  1.0000, -1.0000, -1.0000,  0.8655,  0.9941,  1.0000,  1.0000,\n",
      "        -1.0000, -1.0000,  1.0000, -0.9181, -0.9197, -1.0000, -1.0000,  0.9997,\n",
      "         1.0000, -0.6430,  0.9972,  0.1168, -1.0000, -1.0000,  0.9998, -1.0000,\n",
      "        -1.0000,  1.0000,  0.1238, -1.0000, -1.0000,  1.0000, -1.0000, -0.9691,\n",
      "        -0.3994,  1.0000,  1.0000, -0.9902, -1.0000,  1.0000, -1.0000, -0.9979,\n",
      "         1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -0.0993, -0.6598,  1.0000,\n",
      "         0.9973,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.9279,  0.9773,\n",
      "         1.0000,  0.9933, -0.9268,  1.0000, -1.0000, -1.0000, -0.9985, -1.0000,\n",
      "         0.9997,  0.9981,  0.9967,  1.0000, -1.0000,  1.0000, -0.9196,  0.9971,\n",
      "         1.0000, -1.0000,  1.0000,  0.9997,  0.0036, -1.0000,  0.9772,  0.9999,\n",
      "         1.0000,  1.0000, -0.9045,  0.9923, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         1.0000, -1.0000, -0.9994,  1.0000, -1.0000,  0.9676,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, -1.0000, -0.3977,  0.6738,  1.0000,  1.0000, -0.9656],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_probs[targets]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Print loss and gradients\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print('Loss:', loss.item())\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Update parameters using gradient descent\u001b[39;00m\n",
      "File \u001b[1;32me:\\pytorch\\venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\pytorch\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\pytorch\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for i in range(inputs.size(0)):\n",
    "    input = inputs[i]\n",
    "    input_to_hidden = torch.matmul(Wxh, input)\n",
    "    hidden_to_hidden = torch.matmul(Whh, hidden)\n",
    "    pre_activation = input_to_hidden + hidden_to_hidden + bh\n",
    "    hidden = torch.tanh(pre_activation)\n",
    "    print({i} ,hidden )\n",
    "\n",
    "    #hidden = torch.tanh(torch.matmul(Wxh, input) + torch.matmul(Whh, hidden) + bh)\n",
    "output = torch.matmul(Why, hidden) + by\n",
    "\n",
    "# Compute loss (negative log likelihood loss)\n",
    "log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "loss = -log_probs[targets]\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Print loss and gradients\n",
    "#print('Loss:', loss.item())\n",
    "\n",
    "# Update parameters using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9023666381835938\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0836],\n",
      "        [0.4906]])\n",
      "Whh.grad: tensor([[ 0.0612,  0.0892],\n",
      "        [ 0.1621, -0.0302]])\n",
      "Why.grad: tensor([[-0.4174, -0.7529],\n",
      "        [ 0.4174,  0.7529]])\n",
      "bh.grad: tensor([-0.3539, -0.5672])\n",
      "by.grad: tensor([ 0.8508, -0.8508])\n",
      "predicted : tensor([ 1.0097, -0.7311], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.8508, 0.1492], grad_fn=<SoftmaxBackward0>)\n",
      "target : tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "output_size = 2\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Sample input (batch size = 1, sequence length = 3, input size = 10)\n",
    "inputs = torch.randn(3, input_size)\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "for i in range(inputs.size(0)):\n",
    "    input = inputs[i]\n",
    "    hidden = torch.tanh(torch.matmul(Wxh, input) + torch.matmul(Whh, hidden) + bh)\n",
    "output = torch.matmul(Why, hidden) + by\n",
    "\n",
    "\n",
    "# Compute loss (negative log likelihood loss)\n",
    "log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "#criterion = torch.nn.MSELoss()\n",
    "#loss = criterion(softmax, target)\n",
    "loss = -log_probs[targets]\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Print loss and gradients\n",
    "print('Loss:', loss.item())\n",
    "print('Gradients:')\n",
    "print('Wxh.grad:', Wxh.grad)\n",
    "print('Whh.grad:', Whh.grad)\n",
    "print('Why.grad:', Why.grad)\n",
    "print('bh.grad:', bh.grad)\n",
    "print('by.grad:', by.grad)\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    Wxh -= learning_rate * Wxh.grad\n",
    "    Whh -= learning_rate * Whh.grad\n",
    "    Why -= learning_rate * Why.grad\n",
    "    bh -= learning_rate * bh.grad\n",
    "    by -= learning_rate * by.grad\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    Wxh.grad.zero_()\n",
    "    Whh.grad.zero_()\n",
    "    Why.grad.zero_()\n",
    "    bh.grad.zero_()\n",
    "    by.grad.zero_()\n",
    "print(\"predicted :\",output)\n",
    "softmax = torch.nn.functional.softmax(output, dim=0)\n",
    "print('Output (softmax probabilities):', softmax)\n",
    "\n",
    "print(\"target :\", targets) \n",
    "#print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target: tensor([1])\n",
      "Epoch 1\n",
      "Output (logits): tensor([-1.4826,  1.4140], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0523, 0.9477], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0027377326041460037\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0135],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7498e-03,  8.7890e-03],\n",
      "        [-2.0461e-05,  9.4991e-05]])\n",
      "Why.grad: tensor([[ 0.0042,  0.0052],\n",
      "        [-0.0042, -0.0052]])\n",
      "bh.grad: tensor([-0.0052, -0.0015])\n",
      "by.grad: tensor([ 0.0052, -0.0052])\n",
      "**************************************************\n",
      "Epoch 2\n",
      "Output (logits): tensor([-1.4829,  1.4145], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0523, 0.9477], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002733332570642233\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0135],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7453e-03,  8.7685e-03],\n",
      "        [-2.0401e-05,  9.4829e-05]])\n",
      "Why.grad: tensor([[ 0.0042,  0.0052],\n",
      "        [-0.0042, -0.0052]])\n",
      "bh.grad: tensor([-0.0052, -0.0015])\n",
      "by.grad: tensor([ 0.0052, -0.0052])\n",
      "**************************************************\n",
      "Epoch 3\n",
      "Output (logits): tensor([-1.4833,  1.4150], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0522, 0.9478], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0027289451099932194\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0134],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7408e-03,  8.7482e-03],\n",
      "        [-2.0341e-05,  9.4669e-05]])\n",
      "Why.grad: tensor([[ 0.0042,  0.0052],\n",
      "        [-0.0042, -0.0052]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0052, -0.0052])\n",
      "**************************************************\n",
      "Epoch 4\n",
      "Output (logits): tensor([-1.4837,  1.4154], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0522, 0.9478], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002724575577303767\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0134],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7363e-03,  8.7280e-03],\n",
      "        [-2.0282e-05,  9.4509e-05]])\n",
      "Why.grad: tensor([[ 0.0042,  0.0051],\n",
      "        [-0.0042, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0052, -0.0052])\n",
      "**************************************************\n",
      "Epoch 5\n",
      "Output (logits): tensor([-1.4840,  1.4159], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0522, 0.9478], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002720228396356106\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0134],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7319e-03,  8.7078e-03],\n",
      "        [-2.0223e-05,  9.4350e-05]])\n",
      "Why.grad: tensor([[ 0.0042,  0.0051],\n",
      "        [-0.0042, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0052, -0.0052])\n",
      "**************************************************\n",
      "Epoch 6\n",
      "Output (logits): tensor([-1.4844,  1.4164], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0521, 0.9479], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002715896815061569\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0133],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7274e-03,  8.6878e-03],\n",
      "        [-2.0164e-05,  9.4191e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 7\n",
      "Output (logits): tensor([-1.4848,  1.4169], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0521, 0.9479], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0027115843258798122\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0133],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7230e-03,  8.6679e-03],\n",
      "        [-2.0105e-05,  9.4033e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 8\n",
      "Output (logits): tensor([-1.4852,  1.4173], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0520, 0.9480], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0027072899974882603\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0133],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7186e-03,  8.6480e-03],\n",
      "        [-2.0047e-05,  9.3876e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 9\n",
      "Output (logits): tensor([-1.4855,  1.4178], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0520, 0.9480], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0027030124329030514\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0133],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7141e-03,  8.6283e-03],\n",
      "        [-1.9989e-05,  9.3719e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 10\n",
      "Output (logits): tensor([-1.4859,  1.4183], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0519, 0.9481], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026987497694790363\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0132],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7098e-03,  8.6086e-03],\n",
      "        [-1.9931e-05,  9.3564e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 11\n",
      "Output (logits): tensor([-1.4863,  1.4187], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0519, 0.9481], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002694506663829088\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0132],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7054e-03,  8.5890e-03],\n",
      "        [-1.9874e-05,  9.3408e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0051, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 12\n",
      "Output (logits): tensor([-1.4866,  1.4192], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0519, 0.9481], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026902747340500355\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0132],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.7010e-03,  8.5695e-03],\n",
      "        [-1.9816e-05,  9.3253e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 13\n",
      "Output (logits): tensor([-1.4870,  1.4196], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0518, 0.9482], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026860679499804974\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0131],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6967e-03,  8.5501e-03],\n",
      "        [-1.9759e-05,  9.3099e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 14\n",
      "Output (logits): tensor([-1.4873,  1.4201], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0518, 0.9482], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002681870711967349\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0131],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6924e-03,  8.5308e-03],\n",
      "        [-1.9703e-05,  9.2946e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 15\n",
      "Output (logits): tensor([-1.4877,  1.4206], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0517, 0.9483], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026776900049299\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0131],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6881e-03,  8.5115e-03],\n",
      "        [-1.9646e-05,  9.2793e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0051],\n",
      "        [-0.0041, -0.0051]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 16\n",
      "Output (logits): tensor([-1.4881,  1.4210], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0517, 0.9483], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026735307183116674\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0130],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6838e-03,  8.4924e-03],\n",
      "        [-1.9590e-05,  9.2641e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 17\n",
      "Output (logits): tensor([-1.4884,  1.4215], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0517, 0.9483], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002669384703040123\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0130],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6795e-03,  8.4733e-03],\n",
      "        [-1.9534e-05,  9.2489e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 18\n",
      "Output (logits): tensor([-1.4888,  1.4219], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0516, 0.9484], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002665255218744278\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0130],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6752e-03,  8.4544e-03],\n",
      "        [-1.9478e-05,  9.2338e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0051, -0.0051])\n",
      "**************************************************\n",
      "Epoch 19\n",
      "Output (logits): tensor([-1.4891,  1.4224], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0516, 0.9484], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026611483190208673\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0130],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6710e-03,  8.4355e-03],\n",
      "        [-1.9423e-05,  9.2187e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 20\n",
      "Output (logits): tensor([-1.4895,  1.4228], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0515, 0.9485], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002657046541571617\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0129],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6667e-03,  8.4167e-03],\n",
      "        [-1.9368e-05,  9.2038e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 21\n",
      "Output (logits): tensor([-1.4899,  1.4233], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0515, 0.9485], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002652971539646387\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0129],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6625e-03,  8.3980e-03],\n",
      "        [-1.9313e-05,  9.1889e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0050, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 22\n",
      "Output (logits): tensor([-1.4902,  1.4238], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0515, 0.9485], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002648904686793685\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0129],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6583e-03,  8.3793e-03],\n",
      "        [-1.9258e-05,  9.1740e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 23\n",
      "Output (logits): tensor([-1.4906,  1.4242], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0514, 0.9486], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002644855994731188\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0128],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6541e-03,  8.3608e-03],\n",
      "        [-1.9204e-05,  9.1592e-05]])\n",
      "Why.grad: tensor([[ 0.0041,  0.0050],\n",
      "        [-0.0041, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 24\n",
      "Output (logits): tensor([-1.4909,  1.4247], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0514, 0.9486], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.002640827326104045\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0128],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6500e-03,  8.3423e-03],\n",
      "        [-1.9149e-05,  9.1445e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 25\n",
      "Output (logits): tensor([-1.4913,  1.4251], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0513, 0.9487], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026368130929768085\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0128],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6458e-03,  8.3240e-03],\n",
      "        [-1.9095e-05,  9.1298e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0015])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 26\n",
      "Output (logits): tensor([-1.4916,  1.4255], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0513, 0.9487], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026328079402446747\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0128],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6416e-03,  8.3056e-03],\n",
      "        [-1.9042e-05,  9.1151e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0014])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 27\n",
      "Output (logits): tensor([-1.4920,  1.4260], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0513, 0.9487], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026288218796253204\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0127],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6375e-03,  8.2874e-03],\n",
      "        [-1.8988e-05,  9.1005e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0014])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 28\n",
      "Output (logits): tensor([-1.4923,  1.4264], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0512, 0.9488], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.00262485072016716\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0127],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6334e-03,  8.2693e-03],\n",
      "        [-1.8935e-05,  9.0860e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0014])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 29\n",
      "Output (logits): tensor([-1.4927,  1.4269], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0512, 0.9488], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.0026208944618701935\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0127],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6293e-03,  8.2512e-03],\n",
      "        [-1.8882e-05,  9.0716e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0050],\n",
      "        [-0.0040, -0.0050]])\n",
      "bh.grad: tensor([-0.0049, -0.0014])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n",
      "Epoch 30\n",
      "Output (logits): tensor([-1.4930,  1.4273], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([0.0512, 0.9488], grad_fn=<SoftmaxBackward0>)\n",
      "One-hot encoded target: tensor([0., 1.])\n",
      "Loss: 0.00261695496737957\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[0.0126],\n",
      "        [0.0015]])\n",
      "Whh.grad: tensor([[ 2.6252e-03,  8.2332e-03],\n",
      "        [-1.8829e-05,  9.0572e-05]])\n",
      "Why.grad: tensor([[ 0.0040,  0.0049],\n",
      "        [-0.0040, -0.0049]])\n",
      "bh.grad: tensor([-0.0049, -0.0014])\n",
      "by.grad: tensor([ 0.0050, -0.0050])\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample sizes for demonstration\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "output_size = 2\n",
    "\n",
    "# Initialize parameters\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Sample input and initial hidden state\n",
    "input = torch.randn(3,input_size)\n",
    "targets = torch.tensor([1], dtype=torch.long)  # Sample target\n",
    "print(' target:', targets)\n",
    "l=[]\n",
    "for epoch in range(30):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "    for i in range(inputs.size(0)):\n",
    "        input = inputs[i]\n",
    "        hidden = torch.tanh(torch.matmul(Wxh, input) + torch.matmul(Whh, hidden) + bh)\n",
    "    output = torch.matmul(Why, hidden) + by\n",
    "\n",
    "\n",
    "# Print output (logits)\n",
    "    print('Output (logits):', output)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "    softmax = torch.nn.functional.softmax(output, dim=0)\n",
    "    print('Output (softmax probabilities):', softmax)\n",
    "\n",
    "# Define target\n",
    "\n",
    "# Convert target to one-hot encoding\n",
    "    one_hot_target = torch.zeros(output_size)\n",
    "    one_hot_target[targets] = 1.0\n",
    "    print('One-hot encoded target:', one_hot_target)\n",
    "\n",
    "# Compute loss using MSE\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss = criterion(softmax, one_hot_target)\n",
    "    print('Loss:', loss.item())\n",
    "    l.append(loss.item())\n",
    "\n",
    "# Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "    print('Gradients:')\n",
    "    print('Wxh.grad:', Wxh.grad)\n",
    "    print('Whh.grad:', Whh.grad)\n",
    "    print('Why.grad:', Why.grad)\n",
    "    print('bh.grad:', bh.grad)\n",
    "    print('by.grad:', by.grad)\n",
    "    with torch.no_grad():\n",
    "        Wxh -= learning_rate * Wxh.grad\n",
    "        Whh -= learning_rate * Whh.grad\n",
    "        Why -= learning_rate * Why.grad\n",
    "        bh -= learning_rate * bh.grad\n",
    "        by -= learning_rate * by.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        Wxh.grad.zero_()\n",
    "        Whh.grad.zero_()\n",
    "        Why.grad.zero_()\n",
    "        bh.grad.zero_()\n",
    "        by.grad.zero_()\n",
    "    \n",
    "    # Print a separator between epochs\n",
    "    print('*' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0027377326041460037,\n",
       " 0.002733332570642233,\n",
       " 0.0027289451099932194,\n",
       " 0.002724575577303767,\n",
       " 0.002720228396356106,\n",
       " 0.002715896815061569,\n",
       " 0.0027115843258798122,\n",
       " 0.0027072899974882603,\n",
       " 0.0027030124329030514,\n",
       " 0.0026987497694790363,\n",
       " 0.002694506663829088,\n",
       " 0.0026902747340500355,\n",
       " 0.0026860679499804974,\n",
       " 0.002681870711967349,\n",
       " 0.0026776900049299,\n",
       " 0.0026735307183116674,\n",
       " 0.002669384703040123,\n",
       " 0.002665255218744278,\n",
       " 0.0026611483190208673,\n",
       " 0.002657046541571617,\n",
       " 0.002652971539646387,\n",
       " 0.002648904686793685,\n",
       " 0.002644855994731188,\n",
       " 0.002640827326104045,\n",
       " 0.0026368130929768085,\n",
       " 0.0026328079402446747,\n",
       " 0.0026288218796253204,\n",
       " 0.00262485072016716,\n",
       " 0.0026208944618701935,\n",
       " 0.00261695496737957]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'z')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase)}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Name Tensor: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Sample input (batch size = 1, sequence length = 3, input size = 10)\n",
    "inputs = torch.randn(3, input_size)\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "# Encode your name\n",
    "name = \"my\"\n",
    "name1=\"name\"\n",
    "name2=\"is\"\n",
    "encoded_name11 = encode_string(name, input_size)\n",
    "encoded_name1 = encode_string(name1, input_size)\n",
    "encoded_name2 = encode_string(name2, input_size)\n",
    "encoded_name=[encoded_name11,encoded_name1,encoded_name11]\n",
    "print(f'Encoded Name Tensor: {encoded_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Name Tensor: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'z')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase)}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Define input size\n",
    "input_size = 10\n",
    "\n",
    "# Encode each part of the name\n",
    "name = \"my\"\n",
    "name1 = \"name\"\n",
    "name2 = \"is\"\n",
    "\n",
    "encoded_name1 = encode_string(name, input_size)\n",
    "encoded_name2 = encode_string(name1, input_size)\n",
    "encoded_name3 = encode_string(name2, input_size)\n",
    "\n",
    "# Combine encoded names into a single tensor\n",
    "encoded_names = torch.stack([encoded_name1, encoded_name2, encoded_name3])\n",
    "\n",
    "print(f'Encoded Name Tensor: {encoded_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3792,  0.2564,  0.4362,  0.0084, -0.2629,  1.7328, -0.8581,  1.4739,\n",
       "          0.4148,  1.5389],\n",
       "        [ 0.3158,  0.2134, -0.6691, -0.7064,  0.3932, -0.1926, -0.9290,  0.3245,\n",
       "         -0.8402, -1.4816],\n",
       "        [ 0.3332, -0.0042, -0.1377,  0.1573, -0.3368, -0.4701,  0.6020,  1.9791,\n",
       "          0.9969,  1.3810]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.2630202770233154\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[ 1.0172e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.1288e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.0386e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-2.4652e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -8.5949e-04, -0.0000e+00],\n",
      "        [-8.3514e-01, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 4.0371e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Whh.grad: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0006, -0.0006,  0.0007,  ...,  0.0003, -0.0007,  0.0004],\n",
      "        [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Why.grad: tensor([[ 0.8960, -0.8960, -0.8960,  0.5848, -0.8960,  0.8843, -0.8446, -0.4229,\n",
      "         -0.8960,  0.8959,  0.7646,  0.8957,  0.8960, -0.8960, -0.8925, -0.8960,\n",
      "         -0.8256, -0.8960,  0.8775, -0.8655, -0.8650,  0.8960,  0.8960,  0.8960,\n",
      "          0.8960, -0.8959, -0.8960, -0.8823,  0.8960, -0.8237,  0.8960,  0.8960,\n",
      "          0.8960, -0.8960,  0.8960,  0.2347, -0.8959, -0.8960, -0.4515,  0.8959,\n",
      "         -0.8959, -0.8960, -0.8958,  0.8960, -0.8954,  0.8296,  0.8960, -0.8960,\n",
      "         -0.8958,  0.8960,  0.8960,  0.8575, -0.8960,  0.8944, -0.8957,  0.6006,\n",
      "         -0.8960,  0.8960,  0.3990, -0.8956, -0.8960,  0.7527,  0.8959,  0.8960,\n",
      "          0.3935,  0.8938, -0.8960,  0.8960,  0.5735, -0.8946, -0.8960, -0.8953,\n",
      "          0.8960, -0.8960,  0.8960, -0.4557,  0.7741,  0.8960, -0.8244,  0.6743,\n",
      "         -0.6217,  0.8960,  0.8960, -0.8951,  0.3517, -0.8930, -0.8959,  0.6912,\n",
      "         -0.8917, -0.8852, -0.8960,  0.8960, -0.8960,  0.8960,  0.1075, -0.8960,\n",
      "         -0.1681,  0.6877, -0.8959,  0.8960,  0.8322, -0.8960, -0.8959,  0.8960,\n",
      "          0.7206, -0.8957,  0.7921, -0.8883,  0.8883,  0.8960, -0.8956,  0.8919,\n",
      "         -0.8959,  0.8736,  0.8960,  0.8960,  0.6425, -0.8719, -0.8956, -0.8960,\n",
      "         -0.8960, -0.8960,  0.8953,  0.8959,  0.8960,  0.8957,  0.8960, -0.8960],\n",
      "        [-0.8960,  0.8960,  0.8960, -0.5848,  0.8960, -0.8843,  0.8446,  0.4229,\n",
      "          0.8960, -0.8959, -0.7646, -0.8957, -0.8960,  0.8960,  0.8925,  0.8960,\n",
      "          0.8256,  0.8960, -0.8775,  0.8655,  0.8650, -0.8960, -0.8960, -0.8960,\n",
      "         -0.8960,  0.8959,  0.8960,  0.8823, -0.8960,  0.8237, -0.8960, -0.8960,\n",
      "         -0.8960,  0.8960, -0.8960, -0.2347,  0.8959,  0.8960,  0.4515, -0.8959,\n",
      "          0.8959,  0.8960,  0.8958, -0.8960,  0.8954, -0.8296, -0.8960,  0.8960,\n",
      "          0.8958, -0.8960, -0.8960, -0.8575,  0.8960, -0.8944,  0.8957, -0.6006,\n",
      "          0.8960, -0.8960, -0.3990,  0.8956,  0.8960, -0.7527, -0.8959, -0.8960,\n",
      "         -0.3935, -0.8938,  0.8960, -0.8960, -0.5735,  0.8946,  0.8960,  0.8953,\n",
      "         -0.8960,  0.8960, -0.8960,  0.4557, -0.7741, -0.8960,  0.8244, -0.6743,\n",
      "          0.6217, -0.8960, -0.8960,  0.8951, -0.3517,  0.8930,  0.8959, -0.6912,\n",
      "          0.8917,  0.8852,  0.8960, -0.8960,  0.8960, -0.8960, -0.1075,  0.8960,\n",
      "          0.1681, -0.6877,  0.8959, -0.8960, -0.8322,  0.8960,  0.8959, -0.8960,\n",
      "         -0.7206,  0.8957, -0.7921,  0.8883, -0.8883, -0.8960,  0.8956, -0.8919,\n",
      "          0.8959, -0.8736, -0.8960, -0.8960, -0.6425,  0.8719,  0.8956,  0.8960,\n",
      "          0.8960,  0.8960, -0.8953, -0.8959, -0.8960, -0.8957, -0.8960,  0.8960]])\n",
      "bh.grad: tensor([-25.8980, -33.5869,  10.6164,   9.3051,  -6.4246, -62.2783, -10.8247,\n",
      "         53.2476, -57.9113, -50.0820,  -3.0569, -13.8173,  27.3216,  -9.9454,\n",
      "        -15.1882,   3.9915,  11.0378,   7.5813,  35.6648, -24.3669, -25.1025,\n",
      "        -55.1953,  29.8077,   9.1069, -18.2185,  31.9364, -23.0034,  34.7801,\n",
      "         -6.7902, -65.2807,  -9.9252, -21.4227,  22.8577, -33.9829,   9.3050,\n",
      "          3.8476, -29.0436, -21.5246,  12.3912,  44.0321, -19.3624,  22.9678,\n",
      "        -14.4643, -33.6910,  65.3918,   7.3369, -12.5484, -47.2206,  -8.2490,\n",
      "         20.1707,   2.1154, -12.2661,  12.3189, -28.3458, -33.4765, -22.5772,\n",
      "        -26.8919, -30.0209,  32.7056,   3.3636, -33.0032,  32.1735,   8.8414,\n",
      "        -10.8730,  30.3389,  -3.5573, -17.1087, -15.6558, -22.1371,  33.5415,\n",
      "         40.6922,  27.7017, -12.5821,  -0.2803,  23.3795,  37.7583,  11.7950,\n",
      "         -0.2941,  17.8771,  28.4380,  21.3454,  29.5010,  20.9910,  55.1677,\n",
      "         -6.8033, -15.8648, -33.8323,  11.2213,   3.2875,  18.3293, -10.7072,\n",
      "         47.1440,  26.1024, -21.4235, -40.3481, -17.9209, -31.2575, -11.5954,\n",
      "         31.1394,  17.6090, -10.5317,  16.3069,  33.1837,  -3.2599, -14.8871,\n",
      "         -5.8212,  18.3328,  16.6212, -31.5677,  25.6768,   9.9426, -19.5600,\n",
      "         18.8119,  -5.2505,  11.6619, -55.7414,  15.9083,  -2.0688, -53.4616,\n",
      "         -6.3756, -59.1158,  25.0490,  39.9388,   6.7617, -31.6577, -21.6694,\n",
      "        -20.7129,  10.0667])\n",
      "by.grad: tensor([ 0.8960, -0.8960])\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Loss: -0.0\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[ 4.5195e-18,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          7.4639e-20,  0.0000e+00],\n",
      "        [ 2.8656e-15,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.8193e-14,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          7.1210e-16,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.0519e-18,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -8.4687e-21,  0.0000e+00],\n",
      "        [-3.0147e-15,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          1.0853e-21,  0.0000e+00],\n",
      "        [-7.0046e-21,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          1.0108e-18,  0.0000e+00]])\n",
      "Whh.grad: tensor([[ 1.0703e-18,  1.5332e-18, -4.6005e-19,  ...,  1.0389e-18,\n",
      "          8.5038e-19, -5.2807e-19],\n",
      "        [ 7.2597e-16,  9.2783e-16, -3.0308e-16,  ...,  6.1141e-16,\n",
      "          5.8520e-16, -2.8750e-16],\n",
      "        [-7.8545e-15, -8.4617e-15,  3.1533e-15,  ..., -5.3033e-15,\n",
      "         -6.4500e-15,  2.1165e-15],\n",
      "        ...,\n",
      "        [-2.5803e-19, -3.4853e-19,  1.0922e-19,  ..., -2.3291e-19,\n",
      "         -2.0659e-19,  1.1401e-19],\n",
      "        [-7.6374e-16, -9.7610e-16,  3.1885e-16,  ..., -6.4322e-16,\n",
      "         -6.1565e-16,  3.0245e-16],\n",
      "        [-1.0125e-18,  9.4418e-19,  2.4408e-19,  ...,  1.0093e-18,\n",
      "         -9.8442e-19, -1.0101e-18]])\n",
      "Why.grad: tensor([[ 2.1334e-14, -2.1335e-14, -2.0983e-14,  1.8964e-15, -2.1335e-14,\n",
      "          2.1333e-14, -2.1335e-14, -2.1335e-14, -2.1335e-14,  2.1332e-14,\n",
      "          2.1335e-14,  2.1335e-14, -3.0645e-15,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1335e-14,  2.1314e-14,  1.9621e-14,  2.1335e-14,  8.9720e-15,\n",
      "          2.1335e-14, -2.1335e-14,  2.1335e-14, -2.1038e-14,  2.1335e-14,\n",
      "         -2.1333e-14,  2.0302e-14,  2.1335e-14,  2.1335e-14, -2.1301e-14,\n",
      "         -9.3545e-15,  2.1335e-14,  2.1335e-14, -2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14, -1.7690e-14, -2.1330e-14, -2.1335e-14,\n",
      "          2.1335e-14, -2.1335e-14, -2.1335e-14,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14, -2.0651e-14,  2.1330e-14, -2.1335e-14,\n",
      "          2.1335e-14,  2.1335e-14, -2.1162e-14, -2.0148e-14,  2.1335e-14,\n",
      "          2.0601e-14, -2.1310e-14,  2.1335e-14,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1333e-14, -2.0644e-14, -2.1145e-14,  2.1335e-14, -2.1335e-14,\n",
      "          2.1246e-14, -2.1333e-14, -2.1335e-14, -2.1292e-14, -2.1234e-14,\n",
      "          2.1335e-14,  2.1169e-14,  2.1335e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14,  2.1335e-14, -2.1299e-14, -2.1335e-14, -2.1335e-14,\n",
      "         -2.1316e-14, -2.1335e-14,  2.1335e-14, -2.1335e-14, -2.1335e-14,\n",
      "          2.1335e-14,  1.1146e-14, -1.9473e-14,  2.1335e-14, -2.0430e-14,\n",
      "         -2.1335e-14,  2.1334e-14, -2.0934e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14,  2.1335e-14, -2.1168e-14, -2.1335e-14, -2.1330e-14,\n",
      "         -2.1334e-14,  2.1334e-14, -2.1326e-14,  2.1325e-14,  2.1335e-14,\n",
      "          1.7900e-14, -2.1335e-14,  2.1317e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14,  2.1333e-14,  2.1335e-14, -2.0979e-14,\n",
      "          2.1335e-14,  2.1323e-14,  2.1335e-14,  2.1335e-14,  2.1335e-14,\n",
      "          2.1334e-14, -2.1335e-14,  2.1334e-14,  2.1335e-14,  2.1335e-14,\n",
      "          2.1335e-14, -2.1335e-14, -2.1330e-14],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "bh.grad: tensor([-1.6848e-13, -1.2255e-13,  8.8577e-14, -3.4936e-14,  1.8632e-13,\n",
      "         1.3062e-13, -9.7923e-16, -1.7915e-13,  3.5263e-13,  2.0358e-13,\n",
      "         1.0052e-13, -3.6424e-13,  4.7815e-14,  2.0403e-13, -1.0834e-13,\n",
      "        -9.0777e-14, -3.5187e-14,  8.5280e-15, -8.2193e-14, -1.0630e-13,\n",
      "         1.8135e-13, -1.8291e-13, -6.4717e-14, -7.9312e-15, -7.0948e-14,\n",
      "         2.1914e-14,  8.0031e-14, -9.4124e-14, -2.0460e-13,  9.9008e-14,\n",
      "        -1.5208e-13,  2.3014e-13, -2.9656e-13, -1.3580e-13,  1.6114e-13,\n",
      "         2.4595e-13,  1.1103e-14, -1.0998e-13,  2.2087e-13,  5.1716e-14,\n",
      "        -3.5340e-14, -1.3551e-13, -1.0747e-13,  2.0515e-14, -1.8458e-14,\n",
      "        -1.8346e-14, -1.8137e-13, -3.4802e-14, -1.3225e-13, -1.4222e-13,\n",
      "        -2.0887e-13, -8.1585e-16, -7.2562e-15,  1.0295e-13, -2.5881e-13,\n",
      "         1.2984e-13,  6.5213e-14, -6.6677e-14, -3.3335e-14,  1.3496e-15,\n",
      "        -1.4698e-13, -5.3508e-14,  1.1710e-13,  1.0766e-15, -2.0682e-13,\n",
      "         1.4236e-13,  2.4840e-13, -2.8779e-13, -1.3005e-13,  2.2724e-13,\n",
      "        -1.3629e-13,  1.4264e-13, -1.0204e-13,  2.1824e-13,  3.2968e-14,\n",
      "         4.3900e-13,  2.1754e-13,  8.6716e-15,  1.4102e-13,  2.0537e-13,\n",
      "         2.6936e-13,  4.9907e-14,  1.6635e-13, -3.4669e-14, -2.7874e-14,\n",
      "         2.1976e-14, -8.0521e-15, -1.1759e-13,  6.4113e-14, -1.4660e-13,\n",
      "         1.5952e-13,  2.0628e-13, -1.9093e-13,  5.7018e-15, -1.1079e-13,\n",
      "         3.6881e-13, -1.9751e-13, -1.5793e-13,  2.6719e-13,  1.5296e-13,\n",
      "        -1.0958e-13, -5.9821e-14, -3.2197e-13,  1.4753e-14, -1.1864e-13,\n",
      "        -3.7627e-13,  4.8084e-14, -4.9686e-14,  1.4686e-13, -1.5852e-13,\n",
      "        -1.1072e-13, -2.3470e-13,  2.3515e-14, -1.9326e-13,  1.2923e-13,\n",
      "         5.2054e-15, -4.0686e-13, -9.1590e-14, -1.4253e-13,  4.6703e-14,\n",
      "         2.4521e-13,  1.6085e-13, -5.2173e-13, -1.4568e-14,  5.0333e-14,\n",
      "         6.7710e-14,  8.0197e-14,  1.8032e-13])\n",
      "by.grad: tensor([2.1335e-14, 0.0000e+00])\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Loss: -0.0\n",
      "Gradients:\n",
      "Wxh.grad: tensor([[ 4.5195e-18,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          7.4639e-20,  0.0000e+00],\n",
      "        [ 2.8656e-15,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.8193e-14,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          7.1210e-16,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.0519e-18,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -8.4687e-21,  0.0000e+00],\n",
      "        [-3.0147e-15,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          1.0853e-21,  0.0000e+00],\n",
      "        [-7.0046e-21,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          1.0108e-18,  0.0000e+00]])\n",
      "Whh.grad: tensor([[ 1.0703e-18,  1.5332e-18, -4.6005e-19,  ...,  1.0389e-18,\n",
      "          8.5038e-19, -5.2807e-19],\n",
      "        [ 7.2597e-16,  9.2783e-16, -3.0308e-16,  ...,  6.1141e-16,\n",
      "          5.8520e-16, -2.8750e-16],\n",
      "        [-7.8545e-15, -8.4617e-15,  3.1533e-15,  ..., -5.3033e-15,\n",
      "         -6.4500e-15,  2.1165e-15],\n",
      "        ...,\n",
      "        [-2.5803e-19, -3.4853e-19,  1.0922e-19,  ..., -2.3291e-19,\n",
      "         -2.0659e-19,  1.1401e-19],\n",
      "        [-7.6374e-16, -9.7610e-16,  3.1885e-16,  ..., -6.4322e-16,\n",
      "         -6.1565e-16,  3.0245e-16],\n",
      "        [-1.0125e-18,  9.4418e-19,  2.4408e-19,  ...,  1.0093e-18,\n",
      "         -9.8442e-19, -1.0101e-18]])\n",
      "Why.grad: tensor([[ 2.1334e-14, -2.1335e-14, -2.0983e-14,  1.8964e-15, -2.1335e-14,\n",
      "          2.1333e-14, -2.1335e-14, -2.1335e-14, -2.1335e-14,  2.1332e-14,\n",
      "          2.1335e-14,  2.1335e-14, -3.0645e-15,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1335e-14,  2.1314e-14,  1.9621e-14,  2.1335e-14,  8.9720e-15,\n",
      "          2.1335e-14, -2.1335e-14,  2.1335e-14, -2.1038e-14,  2.1335e-14,\n",
      "         -2.1333e-14,  2.0302e-14,  2.1335e-14,  2.1335e-14, -2.1301e-14,\n",
      "         -9.3545e-15,  2.1335e-14,  2.1335e-14, -2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14, -1.7690e-14, -2.1330e-14, -2.1335e-14,\n",
      "          2.1335e-14, -2.1335e-14, -2.1335e-14,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14, -2.0651e-14,  2.1330e-14, -2.1335e-14,\n",
      "          2.1335e-14,  2.1335e-14, -2.1162e-14, -2.0148e-14,  2.1335e-14,\n",
      "          2.0601e-14, -2.1310e-14,  2.1335e-14,  2.1335e-14, -2.1335e-14,\n",
      "         -2.1333e-14, -2.0644e-14, -2.1145e-14,  2.1335e-14, -2.1335e-14,\n",
      "          2.1246e-14, -2.1333e-14, -2.1335e-14, -2.1292e-14, -2.1234e-14,\n",
      "          2.1335e-14,  2.1169e-14,  2.1335e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14,  2.1335e-14, -2.1299e-14, -2.1335e-14, -2.1335e-14,\n",
      "         -2.1316e-14, -2.1335e-14,  2.1335e-14, -2.1335e-14, -2.1335e-14,\n",
      "          2.1335e-14,  1.1146e-14, -1.9473e-14,  2.1335e-14, -2.0430e-14,\n",
      "         -2.1335e-14,  2.1334e-14, -2.0934e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14,  2.1335e-14, -2.1168e-14, -2.1335e-14, -2.1330e-14,\n",
      "         -2.1334e-14,  2.1334e-14, -2.1326e-14,  2.1325e-14,  2.1335e-14,\n",
      "          1.7900e-14, -2.1335e-14,  2.1317e-14,  2.1335e-14,  2.1335e-14,\n",
      "         -2.1335e-14, -2.1335e-14,  2.1333e-14,  2.1335e-14, -2.0979e-14,\n",
      "          2.1335e-14,  2.1323e-14,  2.1335e-14,  2.1335e-14,  2.1335e-14,\n",
      "          2.1334e-14, -2.1335e-14,  2.1334e-14,  2.1335e-14,  2.1335e-14,\n",
      "          2.1335e-14, -2.1335e-14, -2.1330e-14],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "bh.grad: tensor([-1.6848e-13, -1.2255e-13,  8.8577e-14, -3.4936e-14,  1.8632e-13,\n",
      "         1.3062e-13, -9.7923e-16, -1.7915e-13,  3.5263e-13,  2.0358e-13,\n",
      "         1.0052e-13, -3.6424e-13,  4.7815e-14,  2.0403e-13, -1.0834e-13,\n",
      "        -9.0777e-14, -3.5187e-14,  8.5280e-15, -8.2193e-14, -1.0630e-13,\n",
      "         1.8135e-13, -1.8291e-13, -6.4717e-14, -7.9312e-15, -7.0948e-14,\n",
      "         2.1914e-14,  8.0031e-14, -9.4124e-14, -2.0460e-13,  9.9008e-14,\n",
      "        -1.5208e-13,  2.3014e-13, -2.9656e-13, -1.3580e-13,  1.6114e-13,\n",
      "         2.4595e-13,  1.1103e-14, -1.0998e-13,  2.2087e-13,  5.1716e-14,\n",
      "        -3.5340e-14, -1.3551e-13, -1.0747e-13,  2.0515e-14, -1.8458e-14,\n",
      "        -1.8346e-14, -1.8137e-13, -3.4802e-14, -1.3225e-13, -1.4222e-13,\n",
      "        -2.0887e-13, -8.1585e-16, -7.2562e-15,  1.0295e-13, -2.5881e-13,\n",
      "         1.2984e-13,  6.5213e-14, -6.6677e-14, -3.3335e-14,  1.3496e-15,\n",
      "        -1.4698e-13, -5.3508e-14,  1.1710e-13,  1.0766e-15, -2.0682e-13,\n",
      "         1.4236e-13,  2.4840e-13, -2.8779e-13, -1.3005e-13,  2.2724e-13,\n",
      "        -1.3629e-13,  1.4264e-13, -1.0204e-13,  2.1824e-13,  3.2968e-14,\n",
      "         4.3900e-13,  2.1754e-13,  8.6716e-15,  1.4102e-13,  2.0537e-13,\n",
      "         2.6936e-13,  4.9907e-14,  1.6635e-13, -3.4669e-14, -2.7874e-14,\n",
      "         2.1976e-14, -8.0521e-15, -1.1759e-13,  6.4113e-14, -1.4660e-13,\n",
      "         1.5952e-13,  2.0628e-13, -1.9093e-13,  5.7018e-15, -1.1079e-13,\n",
      "         3.6881e-13, -1.9751e-13, -1.5793e-13,  2.6719e-13,  1.5296e-13,\n",
      "        -1.0958e-13, -5.9821e-14, -3.2197e-13,  1.4753e-14, -1.1864e-13,\n",
      "        -3.7627e-13,  4.8084e-14, -4.9686e-14,  1.4686e-13, -1.5852e-13,\n",
      "        -1.1072e-13, -2.3470e-13,  2.3515e-14, -1.9326e-13,  1.2923e-13,\n",
      "         5.2054e-15, -4.0686e-13, -9.1590e-14, -1.4253e-13,  4.6703e-14,\n",
      "         2.4521e-13,  1.6085e-13, -5.2173e-13, -1.4568e-14,  5.0333e-14,\n",
      "         6.7710e-14,  8.0197e-14,  1.8032e-13])\n",
      "by.grad: tensor([2.1335e-14, 0.0000e+00])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'z')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase)}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Encode each part of the name\n",
    "name = \"my\"\n",
    "name1 = \"name\"\n",
    "name2 = \"is\"\n",
    "\n",
    "encoded_name1 = encode_string(name, input_size)\n",
    "encoded_name2 = encode_string(name1, input_size)\n",
    "encoded_name3 = encode_string(name2, input_size)\n",
    "\n",
    "# Combine encoded names into a single tensor (sequence length = 3, input size = 10)\n",
    "encoded_names = torch.stack([encoded_name1, encoded_name2, encoded_name3])\n",
    "\n",
    "# Define target\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "# Training loop for 3 epochs\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    # Forward pass\n",
    "    hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "    \n",
    "    # Pass the encoded names as a sequence\n",
    "    for i in range(encoded_names.size(0)):  # Iterate over the sequence length\n",
    "        input = encoded_names[i]\n",
    "        hidden = torch.tanh(torch.matmul(Wxh, input) + torch.matmul(Whh, hidden) + bh)\n",
    "    \n",
    "    output = torch.matmul(Why, hidden) + by\n",
    "    \n",
    "    # Compute loss (negative log likelihood loss)\n",
    "    log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "    loss = -log_probs[targets]\n",
    "    \n",
    "    # Print loss\n",
    "    print('Loss:', loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print gradients\n",
    "    print('Gradients:')\n",
    "    print('Wxh.grad:', Wxh.grad)\n",
    "    print('Whh.grad:', Whh.grad)\n",
    "    print('Why.grad:', Why.grad)\n",
    "    print('bh.grad:', bh.grad)\n",
    "    print('by.grad:', by.grad)\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    learning_rate = 0.01\n",
    "    with torch.no_grad():\n",
    "        Wxh -= learning_rate * Wxh.grad\n",
    "        Whh -= learning_rate * Whh.grad\n",
    "        Why -= learning_rate * Why.grad\n",
    "        bh -= learning_rate * bh.grad\n",
    "        by -= learning_rate * by.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        Wxh.grad.zero_()\n",
    "        Whh.grad.zero_()\n",
    "        Why.grad.zero_()\n",
    "        bh.grad.zero_()\n",
    "        by.grad.zero_()\n",
    "    \n",
    "    # Print a separator between epochs\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Loss: 0.24127075665940842\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 0.09586779959499836\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 0.06662849274774392\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'j')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10  # Number of unique characters to encode\n",
    "hidden_size = 128\n",
    "output_size = 2  # Number of classes\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Encode text data (multiple sentences for classification)\n",
    "sentences = [\"my\", \"name\", \"is\"]\n",
    "encoded_sentences = [encode_string(sentence, input_size) for sentence in sentences]\n",
    "\n",
    "# Convert encoded sentences into a tensor (sequence length = 3, input size = 10)\n",
    "encoded_sentences_tensor = torch.stack(encoded_sentences)\n",
    "\n",
    "# Define targets for each sentence\n",
    "# Example targets: [0, 1, 1] (for simplicity, use two classes)\n",
    "targets = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "\n",
    "# Training loop for 3 epochs\n",
    "learning_rate = 0.01\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(encoded_sentences_tensor.size(0)):  # Iterate over each sentence\n",
    "        # Get the input and target for this example\n",
    "        input_seq = encoded_sentences_tensor[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        # Forward pass through the sequence\n",
    "        input_t = input_seq  # Directly use the entire tensor\n",
    "        hidden = torch.tanh(torch.matmul(Wxh, input_t) + torch.matmul(Whh, hidden) + bh)\n",
    "        \n",
    "        output = torch.matmul(Why, hidden) + by\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            Wxh -= learning_rate * Wxh.grad\n",
    "            Whh -= learning_rate * Whh.grad\n",
    "            Why -= learning_rate * Why.grad\n",
    "            bh -= learning_rate * bh.grad\n",
    "            by -= learning_rate * by.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            Wxh.grad.zero_()\n",
    "            Whh.grad.zero_()\n",
    "            Why.grad.zero_()\n",
    "            bh.grad.zero_()\n",
    "            by.grad.zero_()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / encoded_sentences_tensor.size(0)}')\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'j')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Function to encode a list of words into a tensor\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    # Pad or truncate to the sequence length\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        # Pad with zeros if shorter\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        # Truncate if longer\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    \n",
    "    return torch.stack(encoded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Convert encoded sentences into a tensor\n",
    "encoded_sentences_tensor = torch.stack(encoded_sentences)\n",
    "\n",
    "# Define targets for each sentence\n",
    "# Example targets: [0, 1, 1] (for simplicity, use two classes)\n",
    "targets = torch.tensor([0, 1, 1], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 1., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "loss tensor(6.5298, grad_fn=<NegBackward0>)\n",
      "total_loss 6.529806613922119\n",
      "loss tensor(18.9923, grad_fn=<NegBackward0>)\n",
      "total_loss 25.522078037261963\n",
      "loss tensor(8.8214e-06, grad_fn=<NegBackward0>)\n",
      "total_loss 25.52208685871028\n",
      "Average Loss: 8.507362286236761\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "loss tensor(7.3013, grad_fn=<NegBackward0>)\n",
      "total_loss 7.3013105392456055\n",
      "loss tensor(5.4234, grad_fn=<NegBackward0>)\n",
      "total_loss 12.724719524383545\n",
      "loss tensor(0.0053, grad_fn=<NegBackward0>)\n",
      "total_loss 12.730038775596768\n",
      "Average Loss: 4.243346258532256\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "loss tensor(7.5698, grad_fn=<NegBackward0>)\n",
      "total_loss 7.569842338562012\n",
      "loss tensor(4.4424, grad_fn=<NegBackward0>)\n",
      "total_loss 12.012202262878418\n",
      "loss tensor(0.0079, grad_fn=<NegBackward0>)\n",
      "total_loss 12.020064242184162\n",
      "Average Loss: 4.006688080728054\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights and biases\n",
    "Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Training loop for 3 epochs\n",
    "learning_rate = 0.01\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(encoded_sentences_tensor.size(0)):  # Iterate over each sentence\n",
    "        # Get the input and target for this example\n",
    "        input_seq = encoded_sentences_tensor[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        # Forward pass through the sequence\n",
    "        for t in range(input_seq.size(0)):  # Iterate over sequence length\n",
    "            input_t = input_seq[t]\n",
    "            hidden = torch.tanh(torch.matmul(Wxh, input_t) + torch.matmul(Whh, hidden) + bh)\n",
    "        \n",
    "        output = torch.matmul(Why, hidden) + by\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        print(\"loss\",loss)\n",
    "        total_loss += loss.item()\n",
    "        print(\"total_loss\",total_loss)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            Wxh -= learning_rate * Wxh.grad\n",
    "            Whh -= learning_rate * Whh.grad\n",
    "            Why -= learning_rate * Why.grad\n",
    "            bh -= learning_rate * bh.grad\n",
    "            by -= learning_rate * by.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            Wxh.grad.zero_()\n",
    "            Whh.grad.zero_()\n",
    "            Why.grad.zero_()\n",
    "            bh.grad.zero_()\n",
    "            by.grad.zero_()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / encoded_sentences_tensor.size(0)}')\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.zeros(hidden_size, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Loss: 10.299740561594566\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 17.787879943847656\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 8.810731216100976\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    # Create a mapping from characters to indices (limited to 'a' to 'j')\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    \n",
    "    # Initialize input tensor with zeros\n",
    "    tensor = torch.zeros(input_size)\n",
    "    \n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    # Pad or truncate to the sequence length\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        # Pad with zeros if shorter\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        # Truncate if longer\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    \n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "        self.Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        self.bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(self.hidden_size)  # Initialize hidden state\n",
    "        \n",
    "        for t in range(x.size(0)):  # Iterate over sequence length\n",
    "            input_t = x[t]\n",
    "            hidden = torch.tanh(torch.matmul(self.Wxh, input_t) + torch.matmul(self.Whh, hidden) + self.bh)\n",
    "        \n",
    "        output = torch.matmul(self.Why, hidden) + self.by\n",
    "        return output\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wxh.grad is not None:\n",
    "            self.Wxh.grad.zero_()\n",
    "        if self.Whh.grad is not None:\n",
    "            self.Whh.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.bh.grad is not None:\n",
    "            self.bh.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wxh, self.Whh, self.Why, self.bh, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 3  # Number of classes\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence (e.g., 0 = greeting, 1 = introduction, 2 = statement)\n",
    "targets = torch.tensor([0, 1, 2], dtype=torch.long)  # Adjusted to match the number of sentences\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "wxh tensor([[ 0.4365, -0.4977,  0.3937,  ..., -0.1221, -0.6119, -0.9862],\n",
      "        [-1.9510,  0.2655,  0.4681,  ...,  0.1496,  1.3187,  1.2480],\n",
      "        [-0.7570,  0.8856,  0.4554,  ..., -0.6867,  1.1538, -0.3957],\n",
      "        ...,\n",
      "        [ 0.4818, -0.0393, -0.6965,  ...,  0.3384,  0.4743,  1.1404],\n",
      "        [ 0.3436, -0.8609, -0.5389,  ..., -0.0323,  3.0402,  0.8536],\n",
      "        [-1.2327,  0.8745, -0.7665,  ...,  1.9361,  3.4848,  0.6888]],\n",
      "       requires_grad=True)\n",
      "whh tensor([[ 0.2403,  0.2493,  0.2357,  ...,  1.0187,  0.9711,  0.4949],\n",
      "        [-0.5316,  0.8071,  1.2225,  ..., -0.4071, -1.4822,  0.3550],\n",
      "        [ 0.2834,  0.2744,  0.4535,  ...,  1.3971,  0.5483,  1.1132],\n",
      "        ...,\n",
      "        [ 0.6327,  0.4803, -0.6100,  ..., -1.6567,  2.1380,  1.0752],\n",
      "        [-0.1105,  0.6010,  0.0946,  ..., -1.0506, -0.7140,  1.9608],\n",
      "        [ 1.1007,  0.5505, -0.6447,  ...,  2.1058, -0.2094,  0.9978]],\n",
      "       requires_grad=True)\n",
      "why tensor([[ 0.5969,  0.1008, -0.3983, -0.7073, -1.0111, -2.0861,  0.4242, -1.3914,\n",
      "         -0.2969,  1.0619,  1.3243,  0.9321,  1.3275,  1.0061,  1.1457,  0.5783,\n",
      "         -1.0224,  0.8240, -0.0304,  0.6639,  1.6566,  0.2283, -0.1115, -1.2722,\n",
      "          1.4690,  0.5415, -0.7184, -1.1470,  0.3027,  0.2139, -0.0160,  1.8008,\n",
      "          0.6898,  0.0077, -0.9266, -0.5077, -1.5454, -0.5849, -0.9033, -0.2292,\n",
      "          0.2413,  1.0643,  0.2453, -0.3248, -0.3675,  0.0773,  0.1722, -0.9606,\n",
      "         -0.1396,  0.8241,  0.8180, -0.1316, -0.2967,  0.9740, -0.1591, -1.1293,\n",
      "          0.1425,  0.8422, -0.6693, -0.5833, -1.2314, -1.4587, -1.1053, -0.9259,\n",
      "          0.0868,  0.5728, -1.2890,  1.4752,  0.5586,  0.7409, -0.5548, -0.2545,\n",
      "         -0.0223,  0.5349, -0.7806,  0.2564,  1.1378, -0.5340,  1.7287, -0.7658,\n",
      "         -1.0477, -0.5654,  0.2946, -1.1251, -0.6903, -0.6195,  0.4761, -0.2261,\n",
      "          1.0153, -0.0653, -0.2185, -0.1773, -0.9762,  0.6930,  1.3232, -1.4435,\n",
      "         -1.3798,  0.8749, -0.2108, -0.9082, -0.4363,  2.0564,  0.1012, -0.2325,\n",
      "         -0.3816,  1.0762,  1.3840, -0.2505, -0.1372,  0.6165, -1.4824,  0.8858,\n",
      "          0.6905,  0.9569,  0.9139,  1.1531,  0.2906, -1.3039, -1.0278,  0.9981,\n",
      "         -0.2570,  1.0997, -0.4215,  0.6971,  0.5719,  0.1525, -0.2327, -1.0486],\n",
      "        [ 0.4510,  0.7253,  0.0978, -0.1005,  0.9425,  0.7359, -0.4634, -0.2308,\n",
      "         -1.1063, -1.1128,  0.6369, -0.2969,  0.3431,  1.9982, -0.1327,  0.5434,\n",
      "          0.4295, -1.5429,  0.8859,  0.3032, -0.2321, -0.5196, -0.2172,  0.5729,\n",
      "          0.5440,  0.2269,  0.5455, -0.2692,  0.4001, -0.1623, -0.3712, -1.4491,\n",
      "          0.7030, -1.7556, -0.7960, -0.0073,  0.1578,  0.3886,  1.3503, -1.5850,\n",
      "          0.2192,  0.7437,  2.2609, -0.3522,  1.1928, -0.0838,  1.3345,  0.7327,\n",
      "         -0.0789,  0.8298,  1.1814,  2.3296, -1.1465,  0.7868, -0.5793, -0.2014,\n",
      "          0.4670, -1.1439,  0.2034,  0.8190,  1.0221, -0.0326,  1.3098, -1.2184,\n",
      "          0.5217, -1.9392, -0.7776, -1.3323, -0.1629,  0.8644,  0.5162,  0.4966,\n",
      "          0.2716,  0.7710,  0.1953, -0.5467,  0.4606, -0.3881, -0.5801,  1.0118,\n",
      "         -0.9743, -1.0472, -0.6718,  0.4439,  1.1174,  1.2029,  0.9437,  0.7314,\n",
      "          1.3405, -0.6381,  0.0740, -1.1017,  0.6347,  0.8580,  1.1355, -0.6433,\n",
      "         -1.4967,  1.3275, -0.5088, -1.0227,  0.1757,  0.8056,  0.3143, -0.3241,\n",
      "          0.2523,  0.4304,  1.0581, -0.9443, -0.4614,  2.0001, -0.4040, -0.1012,\n",
      "         -0.8918,  0.3516, -0.4481, -0.4882, -1.2167, -0.4142,  0.1959,  2.0304,\n",
      "          0.1830, -0.3954,  0.3216,  0.5002, -0.0895, -1.2808, -2.2436,  1.5829]],\n",
      "       requires_grad=True)\n",
      "bh tensor([-7.3057e-01,  1.3152e+00, -3.9638e-01, -3.8666e-01,  9.5281e-02,\n",
      "         8.6161e-01, -1.5397e+00,  3.3853e-01,  3.0969e-01, -7.2550e-01,\n",
      "        -1.3834e+00, -1.2656e+00, -6.5931e-02, -1.7327e+00,  3.7394e-01,\n",
      "        -1.1946e+00, -1.7094e+00, -6.6192e-01, -1.5783e+00, -3.2507e-01,\n",
      "         9.2772e-01, -7.7344e-01, -8.6501e-01,  1.0443e-03,  1.0111e-01,\n",
      "         4.4676e-01, -1.3236e+00, -4.2614e-01,  5.4677e-01, -7.2930e-01,\n",
      "         7.2674e-01, -1.7985e+00,  3.4344e-01, -1.5598e+00, -9.5499e-01,\n",
      "        -1.0349e+00,  1.1930e+00,  6.8270e-02,  8.4052e-02,  7.1645e-01,\n",
      "         2.2831e+00,  3.5834e-01, -1.2211e+00, -1.9184e+00, -2.1925e-01,\n",
      "        -1.7873e-01, -4.2340e-01, -9.1099e-01,  5.4353e-01,  1.4820e+00,\n",
      "        -6.9711e-01,  6.0751e-01,  9.1243e-01,  5.0823e-01, -1.2417e+00,\n",
      "         8.5146e-01,  1.5352e+00, -2.8817e-01, -9.3110e-01, -1.3788e+00,\n",
      "        -4.5307e-01,  6.6208e-01,  7.1613e-01, -4.8437e-01,  4.2307e-01,\n",
      "         2.5693e-01,  8.3520e-01,  3.1927e-01,  5.3526e-01,  1.9192e+00,\n",
      "        -8.6529e-01,  5.6961e-01, -9.6468e-01, -7.5115e-01,  1.1967e+00,\n",
      "         7.6503e-01,  3.7686e-01,  6.4888e-03,  5.0206e-02, -1.5197e+00,\n",
      "        -1.0663e+00, -2.5408e-01,  1.2165e+00, -1.1653e+00,  1.2595e+00,\n",
      "         5.8963e-01,  1.7225e+00, -6.8099e-01,  7.2318e-03,  7.6064e-01,\n",
      "         1.6143e+00,  2.7944e-01, -3.7565e-01,  4.8442e-02, -1.4114e+00,\n",
      "         1.2159e+00,  5.7687e-01, -4.8460e-01, -5.1653e-01,  1.0991e+00,\n",
      "        -1.9199e+00, -6.0008e-01, -7.8823e-01,  1.9614e+00, -1.4645e+00,\n",
      "         4.5377e-01,  1.2150e+00,  1.0672e+00,  1.9602e-01, -1.3896e+00,\n",
      "         6.3723e-01, -9.6934e-01,  2.9031e-01, -2.7214e-01, -1.6217e+00,\n",
      "         8.0416e-02,  2.9704e-01, -8.6791e-01,  8.8021e-01,  8.1728e-01,\n",
      "         5.0551e-01, -8.8609e-01, -1.0808e+00, -1.8738e+00,  6.2040e-01,\n",
      "         1.4613e+00,  6.6878e-01,  1.1722e-01], requires_grad=True)\n",
      "by tensor([ 3.8544e-06, -3.8544e-06], requires_grad=True)\n",
      "Epoch 1\n",
      "Average Loss: 8.862436885635057\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 7.047824482123057\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 14.145743211110434\n",
      "--------------------------------------------------\n",
      "Prediction for the new sentence: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        print(\"**********\")\n",
    "        self.Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "        print(\"wxh\",Wxh)\n",
    "        self.Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        print(\"whh\", Whh)\n",
    "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        print(\"why\", Why)\n",
    "        self.bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "        print(\"bh\", bh)\n",
    "        self.by = torch.zeros(output_size, requires_grad=True)\n",
    "        print(\"by\", by)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(self.hidden_size)  # Initialize hidden state\n",
    "        for t in range(x.size(0)):  # Iterate over sequence length\n",
    "            input_t = x[t]\n",
    "            hidden = torch.tanh(torch.matmul(self.Wxh, input_t) + torch.matmul(self.Whh, hidden) + self.bh)\n",
    "        output = torch.matmul(self.Why, hidden) + self.by\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.argmax(output).item()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wxh.grad is not None:\n",
    "            self.Wxh.grad.zero_()\n",
    "        if self.Whh.grad is not None:\n",
    "            self.Whh.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.bh.grad is not None:\n",
    "            self.bh.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wxh, self.Whh, self.Why, self.bh, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 3  # Number of classes\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0, 1, 2], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting new sentence\n",
    "new_sentence = [\"hello\", \"my\", \"name\", \"is\", \"rithin\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Loss: 8.905083119869232\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 16.554217020670574\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 5.586130619049072\n",
      "--------------------------------------------------\n",
      "Prediction for the new sentence: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "        self.Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        self.bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(self.hidden_size,requires_grad=True)  # Initialize hidden state\n",
    "        for t in range(x.size(0)):  # Iterate over sequence length\n",
    "            input_t = x[t]\n",
    "            hidden = torch.tanh(torch.matmul(self.Wxh, input_t) + torch.matmul(self.Whh, hidden) + self.bh)\n",
    "        output = torch.matmul(self.Why, hidden) + self.by\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.argmax(output).item()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wxh.grad is not None:\n",
    "            self.Wxh.grad.zero_()\n",
    "        if self.Whh.grad is not None:\n",
    "            self.Whh.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.bh.grad is not None:\n",
    "            self.bh.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wxh, self.Whh, self.Why, self.bh, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 3  # Number of classes\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0, 1, 2], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting new sentence\n",
    "new_sentence = [\"my\", \"name\", \"is\", \"rithin\",\"a\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new sentence: 0\n"
     ]
    }
   ],
   "source": [
    "new_sentence = [\"hello\", \"my\", \"name\", \"is\", \"rithin\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence =\"my\", \"name\", \"is\", \"rithin\", \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new sentence: 2\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 0.001312467036768794\n",
      "Lossitem: 0.001312467036768794\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 10.984047889709473\n",
      "Lossitem: 10.984047889709473\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 16.403114318847656\n",
      "Lossitem: 16.403114318847656\n",
      "Average Loss: 9.1294915585313\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 25.693758010864258\n",
      "Lossitem: 25.693758010864258\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 1.6910498142242432\n",
      "Lossitem: 1.6910498142242432\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 19.788249969482422\n",
      "Lossitem: 19.788249969482422\n",
      "Average Loss: 15.724352598190308\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 14.565715789794922\n",
      "Lossitem: 14.565715789794922\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 12.336405754089355\n",
      "Lossitem: 12.336405754089355\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Loss: 30.614604949951172\n",
      "Lossitem: 30.614604949951172\n",
      "Average Loss: 19.172242164611816\n",
      "--------------------------------------------------\n",
      "hidin torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "8888\n",
      "by torch.Size([3])\n",
      "why torch.Size([3, 128])\n",
      "hidden torch.Size([128])\n",
      "output torch.Size([3])\n",
      "Prediction for the new sentence: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "        self.Whh = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        self.bh = torch.zeros(hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(self.hidden_size)  # Initialize hidden state\n",
    "        print(\"hidin\",hidden.shape)\n",
    "        for t in range(x.size(0)):  # Iterate over sequence length\n",
    "            input_t = x[t]\n",
    "            hidden = torch.tanh(torch.matmul(self.Wxh, input_t) + torch.matmul(self.Whh, hidden) + self.bh)\n",
    "            print(\"8888\")\n",
    "            print(\"by\",self.by.shape)\n",
    "            print(\"why\",self.Why.shape)\n",
    "            print(\"hidden\", hidden.shape)\n",
    "        output = torch.matmul(self.Why, hidden) + self.by\n",
    "        print(\"output\", output.shape)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.argmax(output).item()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wxh.grad is not None:\n",
    "            self.Wxh.grad.zero_()\n",
    "        if self.Whh.grad is not None:\n",
    "            self.Whh.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.bh.grad is not None:\n",
    "            self.bh.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wxh, self.Whh, self.Why, self.bh, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 3  # Number of classes\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0, 1, 2], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        print(f'Loss: {loss}')\n",
    "        print(f'Lossitem: {loss.item()}')\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting new sentence\n",
    "new_sentence = [\"my\", \"name\", \"is\", \"rithin\",\"a\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
