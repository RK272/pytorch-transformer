{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x10 and 1x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Compute loss (negative log likelihood loss)\u001b[39;00m\n\u001b[0;32m    126\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mSimpleLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m     39\u001b[0m     input_t \u001b[38;5;241m=\u001b[39m x[t]\n\u001b[1;32m---> 41\u001b[0m     i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_t\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUi, h) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbi)\n\u001b[0;32m     42\u001b[0m     f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWf, input_t) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUf, h) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf)\n\u001b[0;32m     43\u001b[0m     o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWo, input_t) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUo, h) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbo)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x10 and 1x10)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "# Define the LSTM class\n",
    "class SimpleLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Weights for LSTM gates\n",
    "        self.Wi = torch.randn(hidden_size, input_size, requires_grad=True)  # Input gate\n",
    "        self.Ui = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.bi = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wf = torch.randn(hidden_size, input_size, requires_grad=True)  # Forget gate\n",
    "        self.Uf = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.bf = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wo = torch.randn(hidden_size, input_size, requires_grad=True)  # Output gate\n",
    "        self.Uo = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.bo = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wc = torch.randn(hidden_size, input_size, requires_grad=True)  # Cell state candidate\n",
    "        self.Uc = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "        self.bc = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "        # Output layer weights\n",
    "        self.Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        seq_len = x.size(0)\n",
    "        h = torch.zeros(self.hidden_size, batch_size)  # Hidden state\n",
    "        c = torch.zeros(self.hidden_size, batch_size)  # Cell state\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_t = x[t]\n",
    "            \n",
    "            i = torch.sigmoid(torch.matmul(self.Wi, input_t) + torch.matmul(self.Ui, h) + self.bi)\n",
    "            f = torch.sigmoid(torch.matmul(self.Wf, input_t) + torch.matmul(self.Uf, h) + self.bf)\n",
    "            o = torch.sigmoid(torch.matmul(self.Wo, input_t) + torch.matmul(self.Uo, h) + self.bo)\n",
    "            c_hat = torch.tanh(torch.matmul(self.Wc, input_t) + torch.matmul(self.Uc, h) + self.bc)\n",
    "            \n",
    "            c = f * c + i * c_hat\n",
    "            h = o * torch.tanh(c)\n",
    "        \n",
    "        output = torch.matmul(self.Why, h) + self.by\n",
    "        return output\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wi, self.Ui, self.bi, self.Wf, self.Uf, self.bf,\n",
    "                self.Wo, self.Uo, self.bo, self.Wc, self.Uc, self.bc,\n",
    "                self.Why, self.by]\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.argmax(output, dim=0).item()\n",
    "\n",
    "# Encode strings as one-hot vectors\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "# Encode a list of words\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10  # Number of unique characters\n",
    "hidden_size = 128\n",
    "output_size = 2  # Number of classes\n",
    "sequence_length = 5  # Define sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i].unsqueeze(1)  # Add batch dimension\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = F.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting a new sentence\n",
    "new_sentence = [\"hello\", \"my\", \"name\", \"is\", \"rithin\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence.unsqueeze(1))  # Add batch dimension\n",
    "print(f'Prediction for the new sentence: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "sequence_length = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "Wi = torch.randn(hidden_size, input_size, requires_grad=True)  # Input gate\n",
    "Ui = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bi = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wf = torch.randn(hidden_size, input_size, requires_grad=True)  # Forget gate\n",
    "Uf = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bf = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wo = torch.randn(hidden_size, input_size, requires_grad=True)  # Output gate\n",
    "Uo = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bo = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wc = torch.randn(hidden_size, input_size, requires_grad=True)  # Cell state candidate\n",
    "Uc = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bc = torch.zeros(hidden_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2881, -0.9919, -0.3783,  ..., -0.7103,  0.8392, -0.6060],\n",
       "        [ 0.1656, -0.7472, -0.8907,  ..., -0.4476,  0.9162, -0.5639],\n",
       "        [ 0.2608, -0.7969, -1.4423,  ..., -1.6837,  0.0633,  1.1479],\n",
       "        ...,\n",
       "        [ 0.5247, -0.1232,  0.3933,  ...,  0.7917, -1.4827,  1.9516],\n",
       "        [ 0.4388, -1.0877, -1.3212,  ...,  1.3887, -0.7424,  0.3653],\n",
       "        [-0.7982, -0.2358,  0.6281,  ...,  0.6494,  0.7852, -0.9230]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(sequence_length, input_size)\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h = torch.zeros(hidden_size, 1)\n",
    "c = torch.zeros(hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5292,  0.0667,  1.3729, -0.7066, -0.1482, -1.2327,  0.9183,  0.7727,\n",
       "          1.1748, -0.8674],\n",
       "        [ 0.3653, -1.0230,  0.5819, -0.0475,  1.1893,  1.7140, -1.4997,  1.8099,\n",
       "         -1.2016,  1.1302],\n",
       "        [ 0.1289, -0.5043,  1.1278,  0.5382, -0.7277,  0.3761, -1.3886, -1.7325,\n",
       "          0.2822, -1.6098],\n",
       "        [-0.4679, -0.9395, -1.5223,  0.7699, -0.7546,  1.0621,  0.1705, -1.2919,\n",
       "         -0.0026,  1.4833],\n",
       "        [ 0.3669,  0.1718,  1.2400, -1.2826,  1.7289, -2.1316,  0.4384, -1.1724,\n",
       "          2.0249,  0.4389]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "for i in range(inputs.size(0)):\n",
    "    input = inputs[i]\n",
    "    hidden = torch.tanh(torch.matmul(Wxh, input) + torch.matmul(Whh, hidden) + bh)\n",
    "output = torch.matmul(Why, hidden) + by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([3, 1])\n",
      "targets: torch.Size([1])\n",
      "hini: torch.Size([1])\n",
      "cini: torch.Size([1])\n",
      "*********\n",
      "t torch.Size([1, 1])\n",
      "wf torch.Size([1, 1])\n",
      "uf torch.Size([1, 1])\n",
      "bf torch.Size([1])\n",
      "++++++++++\n",
      "h: torch.Size([1, 1])\n",
      "c: torch.Size([1, 1])\n",
      "f torch.Size([1, 1])\n",
      "i torch.Size([1, 1])\n",
      "o torch.Size([1, 1])\n",
      "c_hat torch.Size([1, 1])\n",
      "*********\n",
      "t torch.Size([1, 1])\n",
      "wf torch.Size([1, 1])\n",
      "uf torch.Size([1, 1])\n",
      "bf torch.Size([1])\n",
      "++++++++++\n",
      "h: torch.Size([1, 1])\n",
      "c: torch.Size([1, 1])\n",
      "f torch.Size([1, 1])\n",
      "i torch.Size([1, 1])\n",
      "o torch.Size([1, 1])\n",
      "c_hat torch.Size([1, 1])\n",
      "*********\n",
      "t torch.Size([1, 1])\n",
      "wf torch.Size([1, 1])\n",
      "uf torch.Size([1, 1])\n",
      "bf torch.Size([1])\n",
      "++++++++++\n",
      "h: torch.Size([1, 1])\n",
      "c: torch.Size([1, 1])\n",
      "f torch.Size([1, 1])\n",
      "i torch.Size([1, 1])\n",
      "o torch.Size([1, 1])\n",
      "c_hat torch.Size([1, 1])\n",
      "Loss: 1.2640379667282104\n",
      "Gradients:\n",
      "Wi.grad: tensor([[0.0372]])\n",
      "Ui.grad: tensor([[0.0027]])\n",
      "bi.grad: tensor([0.0054])\n",
      "Wf.grad: tensor([[-0.0057]])\n",
      "Uf.grad: tensor([[0.0387]])\n",
      "bf.grad: tensor([-0.1036])\n",
      "Wo.grad: tensor([[-0.0526]])\n",
      "Uo.grad: tensor([[0.0126]])\n",
      "bo.grad: tensor([-0.0480])\n",
      "Wc.grad: tensor([[0.0154]])\n",
      "Uc.grad: tensor([[-0.0420]])\n",
      "bc.grad: tensor([0.1163])\n",
      "Why.grad: tensor([[-0.0979],\n",
      "        [ 0.0979]])\n",
      "by.grad: tensor([-2.9802e-08, -2.9802e-08])\n",
      "predicted : tensor([[-0.0206, -0.0206],\n",
      "        [ 0.1057,  0.1057]], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([[0.4685, 0.4685],\n",
      "        [0.5315, 0.5315]], grad_fn=<SoftmaxBackward0>)\n",
      "target : tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "output_size = 2\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wi = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Ui = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bi = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wf = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uf = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bf = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wo = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uo = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bo = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wc = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uc = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bc = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Sample input (batch size = 1, sequence length = 3, input size = 10)\n",
    "inputs = torch.randn(3, input_size)\n",
    "print(\"inputs:\", inputs.shape)\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "print(\"targets:\", targets.shape)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h = torch.zeros(hidden_size, requires_grad=True)\n",
    "c = torch.zeros(hidden_size, requires_grad=True)\n",
    "print(\"hini:\", h.shape)\n",
    "print(\"cini:\", c.shape)\n",
    "\n",
    "# Forward pass\n",
    "for i in range(inputs.size(0)):\n",
    "    input_t = inputs[i].unsqueeze(1)  # Add batch dimension for the first input in the sequence\n",
    "    print(\"*********\")\n",
    "    print(\"t\",input_t.shape)\n",
    "    print(\"wf\",Wf.shape) \n",
    "    print(\"uf\",Uf.shape)\n",
    "    print(\"bf\",bf.shape)\n",
    "    \n",
    "    print(\"++++++++++\")\n",
    "    f = torch.sigmoid(torch.matmul(Wf, input_t) + torch.matmul(Uf, h) + bf)  # Forget gate\n",
    "    \n",
    "    i = torch.sigmoid(torch.matmul(Wi, input_t) + torch.matmul(Ui, h) + bi)  # Input gate\n",
    "    o = torch.sigmoid(torch.matmul(Wo, input_t) + torch.matmul(Uo, h) + bo)  # Output gate\n",
    "    c_hat = torch.tanh(torch.matmul(Wc, input_t) + torch.matmul(Uc, h) + bc)  # Candidate cell state\n",
    "    \n",
    "    c = f * c + i * c_hat  # Update cell state\n",
    "    h = o * torch.tanh(c)  # Update hidden state\n",
    "    print(\"h:\", h.shape)\n",
    "    print(\"c:\", c.shape)\n",
    "    print(\"f\",f.shape)\n",
    "    print(\"i\", i.shape)\n",
    "    print(\"o\", o.shape)\n",
    "    print(\"c_hat\", c_hat.shape)\n",
    "\n",
    "# Compute output\n",
    "output = torch.matmul(Why, h) + by\n",
    "\n",
    "# Compute loss (negative log likelihood loss)\n",
    "log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "loss = -log_probs[targets].sum()  # Summing the loss to ensure it is a scalar\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Print loss and gradients\n",
    "print('Loss:', loss.item())\n",
    "print('Gradients:')\n",
    "print('Wi.grad:', Wi.grad)\n",
    "print('Ui.grad:', Ui.grad)\n",
    "print('bi.grad:', bi.grad)\n",
    "print('Wf.grad:', Wf.grad)\n",
    "print('Uf.grad:', Uf.grad)\n",
    "print('bf.grad:', bf.grad)\n",
    "print('Wo.grad:', Wo.grad)\n",
    "print('Uo.grad:', Uo.grad)\n",
    "print('bo.grad:', bo.grad)\n",
    "print('Wc.grad:', Wc.grad)\n",
    "print('Uc.grad:', Uc.grad)\n",
    "print('bc.grad:', bc.grad)\n",
    "print('Why.grad:', Why.grad)\n",
    "print('by.grad:', by.grad)\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    Wi -= learning_rate * Wi.grad\n",
    "    Ui -= learning_rate * Ui.grad\n",
    "    bi -= learning_rate * bi.grad\n",
    "    Wf -= learning_rate * Wf.grad\n",
    "    Uf -= learning_rate * Uf.grad\n",
    "    bf -= learning_rate * bf.grad\n",
    "    Wo -= learning_rate * Wo.grad\n",
    "    Uo -= learning_rate * Uo.grad\n",
    "    bo -= learning_rate * bo.grad\n",
    "    Wc -= learning_rate * Wc.grad\n",
    "    Uc -= learning_rate * Uc.grad\n",
    "    bc -= learning_rate * bc.grad\n",
    "    Why -= learning_rate * Why.grad\n",
    "    by -= learning_rate * by.grad\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    Wi.grad.zero_()\n",
    "    Ui.grad.zero_()\n",
    "    bi.grad.zero_()\n",
    "    Wf.grad.zero_()\n",
    "    Uf.grad.zero_()\n",
    "    bf.grad.zero_()\n",
    "    Wo.grad.zero_()\n",
    "    Uo.grad.zero_()\n",
    "    bo.grad.zero_()\n",
    "    Wc.grad.zero_()\n",
    "    Uc.grad.zero_()\n",
    "    bc.grad.zero_()\n",
    "    Why.grad.zero_()\n",
    "    by.grad.zero_()\n",
    "\n",
    "print(\"predicted :\", output)\n",
    "softmax = torch.nn.functional.softmax(output, dim=0)\n",
    "print('Output (softmax probabilities):', softmax)\n",
    "print(\"target :\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[-0.1721, -0.3410, -0.2648],\n",
      "        [-0.5807,  0.2318,  0.3885],\n",
      "        [ 0.1579,  1.0525,  0.6670]]) torch.Size([3, 3])\n",
      "targets: tensor([1]) torch.Size([1])\n",
      "h: tensor([[0.],\n",
      "        [0.]], requires_grad=True) torch.Size([2, 1])\n",
      "c: tensor([[0.],\n",
      "        [0.]], requires_grad=True) torch.Size([2, 1])\n",
      "inputsshape: torch.Size([3, 3])\n",
      "*********\n",
      "input_t {0} tensor([[-0.1721],\n",
      "        [-0.3410],\n",
      "        [-0.2648]]) torch.Size([3, 1])\n",
      "a1: {0} tensor([[0.1080],\n",
      "        [0.9703]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Wf {0} tensor([[ 1.3342,  0.3895, -1.7767],\n",
      "        [-1.2919, -2.1004, -0.1195]], requires_grad=True) torch.Size([2, 3])\n",
      "a2: {0} tensor([[0.],\n",
      "        [0.]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Uf {0} tensor([[ 1.0640,  1.9649],\n",
      "        [-0.5378,  0.4844]], requires_grad=True) torch.Size([2, 2])\n",
      "h {0} tensor([[0.],\n",
      "        [0.]], requires_grad=True) torch.Size([2, 1])\n",
      "bfor {0} <built-in method unsqueeze of Tensor object at 0x000001D9D0C38720>\n",
      "bf {0} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "a3: {0} tensor([[0.1080],\n",
      "        [0.9703]], grad_fn=<AddBackward0>) torch.Size([2, 1])\n",
      "f {0} tensor([[0.5270],\n",
      "        [0.7252]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "Wi {0} tensor([[-0.4751, -1.1386, -0.5288],\n",
      "        [ 0.1026,  0.2969, -0.4905]], requires_grad=True) torch.Size([2, 3])\n",
      "Ui {0} tensor([[-0.7995, -1.2026],\n",
      "        [-0.4998,  0.0195]], requires_grad=True) torch.Size([2, 2])\n",
      "bi {0} tensor([0., 0.], requires_grad=True) torch.Size([2])\n",
      "bi {0} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "h {0} tensor([[0.],\n",
      "        [0.]], requires_grad=True) torch.Size([2, 1])\n",
      "i_gate {0} tensor([[0.6480],\n",
      "        [0.5027]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "h: torch.Size([2, 1])\n",
      "c: torch.Size([2, 1])\n",
      "f torch.Size([2, 1])\n",
      "i torch.Size([2, 1])\n",
      "o torch.Size([2, 1])\n",
      "c_hat torch.Size([2, 1])\n",
      "*********\n",
      "input_t {1} tensor([[-0.5807],\n",
      "        [ 0.2318],\n",
      "        [ 0.3885]]) torch.Size([3, 1])\n",
      "a1: {1} tensor([[-1.3748],\n",
      "        [ 0.2169]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Wf {1} tensor([[ 1.3342,  0.3895, -1.7767],\n",
      "        [-1.2919, -2.1004, -0.1195]], requires_grad=True) torch.Size([2, 3])\n",
      "a2: {1} tensor([[-0.0468],\n",
      "        [ 0.0836]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Uf {1} tensor([[ 1.0640,  1.9649],\n",
      "        [-0.5378,  0.4844]], requires_grad=True) torch.Size([2, 2])\n",
      "h {1} tensor([[-0.1190],\n",
      "        [ 0.0406]], grad_fn=<MulBackward0>) torch.Size([2, 1])\n",
      "bfor {1} <built-in method unsqueeze of Tensor object at 0x000001D9D0C38720>\n",
      "bf {1} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "a3: {1} tensor([[-1.4216],\n",
      "        [ 0.3006]], grad_fn=<AddBackward0>) torch.Size([2, 1])\n",
      "f {1} tensor([[0.1944],\n",
      "        [0.5746]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "Wi {1} tensor([[-0.4751, -1.1386, -0.5288],\n",
      "        [ 0.1026,  0.2969, -0.4905]], requires_grad=True) torch.Size([2, 3])\n",
      "Ui {1} tensor([[-0.7995, -1.2026],\n",
      "        [-0.4998,  0.0195]], requires_grad=True) torch.Size([2, 2])\n",
      "bi {1} tensor([0., 0.], requires_grad=True) torch.Size([2])\n",
      "bi {1} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "h {1} tensor([[-0.1190],\n",
      "        [ 0.0406]], grad_fn=<MulBackward0>) torch.Size([2, 1])\n",
      "i_gate {1} tensor([[0.4633],\n",
      "        [0.4698]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "h: torch.Size([2, 1])\n",
      "c: torch.Size([2, 1])\n",
      "f torch.Size([2, 1])\n",
      "i torch.Size([2, 1])\n",
      "o torch.Size([2, 1])\n",
      "c_hat torch.Size([2, 1])\n",
      "*********\n",
      "input_t {2} tensor([[0.1579],\n",
      "        [1.0525],\n",
      "        [0.6670]]) torch.Size([3, 1])\n",
      "a1: {2} tensor([[-0.5644],\n",
      "        [-2.4945]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Wf {2} tensor([[ 1.3342,  0.3895, -1.7767],\n",
      "        [-1.2919, -2.1004, -0.1195]], requires_grad=True) torch.Size([2, 3])\n",
      "a2: {2} tensor([[ 0.1205],\n",
      "        [-0.0623]], grad_fn=<MmBackward0>) torch.Size([2, 1])\n",
      "Uf {2} tensor([[ 1.0640,  1.9649],\n",
      "        [-0.5378,  0.4844]], requires_grad=True) torch.Size([2, 2])\n",
      "h {2} tensor([[ 0.1149],\n",
      "        [-0.0009]], grad_fn=<MulBackward0>) torch.Size([2, 1])\n",
      "bfor {2} <built-in method unsqueeze of Tensor object at 0x000001D9D0C38720>\n",
      "bf {2} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "a3: {2} tensor([[-0.4438],\n",
      "        [-2.5568]], grad_fn=<AddBackward0>) torch.Size([2, 1])\n",
      "f {2} tensor([[0.3908],\n",
      "        [0.0720]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "Wi {2} tensor([[-0.4751, -1.1386, -0.5288],\n",
      "        [ 0.1026,  0.2969, -0.4905]], requires_grad=True) torch.Size([2, 3])\n",
      "Ui {2} tensor([[-0.7995, -1.2026],\n",
      "        [-0.4998,  0.0195]], requires_grad=True) torch.Size([2, 2])\n",
      "bi {2} tensor([0., 0.], requires_grad=True) torch.Size([2])\n",
      "bi {2} tensor([[0.],\n",
      "        [0.]], grad_fn=<UnsqueezeBackward0>) torch.Size([2, 1])\n",
      "h {2} tensor([[ 0.1149],\n",
      "        [-0.0009]], grad_fn=<MulBackward0>) torch.Size([2, 1])\n",
      "i_gate {2} tensor([[0.1523],\n",
      "        [0.4860]], grad_fn=<SigmoidBackward0>) torch.Size([2, 1])\n",
      "h: torch.Size([2, 1])\n",
      "c: torch.Size([2, 1])\n",
      "f torch.Size([2, 1])\n",
      "i torch.Size([2, 1])\n",
      "o torch.Size([2, 1])\n",
      "c_hat torch.Size([2, 1])\n",
      "Loss: 1.1754319667816162\n",
      "Gradients:\n",
      "Wi.grad: tensor([[ 0.0224, -0.0370, -0.0345],\n",
      "        [ 0.0113,  0.0682,  0.0424]])\n",
      "Ui.grad: tensor([[ 0.0026, -0.0018],\n",
      "        [ 0.0079, -0.0002]])\n",
      "bi.grad: tensor([-0.0698,  0.0678])\n",
      "Wf.grad: tensor([[-0.0078, -0.0123, -0.0055],\n",
      "        [-0.0011,  0.0005,  0.0008]])\n",
      "Uf.grad: tensor([[-2.7350e-03,  4.0463e-04],\n",
      "        [-2.2327e-04,  7.8935e-05]])\n",
      "bf.grad: tensor([-0.0041,  0.0020])\n",
      "Wo.grad: tensor([[ 0.0112, -0.0437, -0.0343],\n",
      "        [ 0.0037,  0.0258,  0.0163]])\n",
      "Uo.grad: tensor([[-9.0364e-04, -1.1030e-03],\n",
      "        [ 2.9064e-03, -2.6019e-05]])\n",
      "bo.grad: tensor([-0.0693,  0.0271])\n",
      "Wc.grad: tensor([[ 0.0462, -0.0280, -0.0370],\n",
      "        [-0.0650, -0.2876, -0.1765]])\n",
      "Uc.grad: tensor([[ 0.0084, -0.0033],\n",
      "        [-0.0346,  0.0015]])\n",
      "bc.grad: tensor([-0.0968, -0.2117])\n",
      "Why.grad: tensor([[ 0.0230, -0.0513],\n",
      "        [-0.0483,  0.1080],\n",
      "        [ 0.0254, -0.0567]])\n",
      "by.grad: tensor([ 0.3285, -0.6913,  0.3628])\n",
      "predicted : tensor([0.1093, 0.0471, 0.2086], grad_fn=<SqueezeBackward1>)\n",
      "Output (softmax probabilities): tensor([0.3285, 0.3087, 0.3628], grad_fn=<SoftmaxBackward0>)\n",
      "target : tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "output_size = 3\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wi = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Ui = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bi = torch.zeros(hidden_size, requires_grad=True)  # Shape: [128]\n",
    "\n",
    "Wf = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uf = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bf = torch.zeros(hidden_size, requires_grad=True)  # Shape: [128]\n",
    "\n",
    "Wo = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uo = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bo = torch.zeros(hidden_size, requires_grad=True)  # Shape: [128]\n",
    "\n",
    "Wc = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uc = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bc = torch.zeros(hidden_size, requires_grad=True)  # Shape: [128]\n",
    "\n",
    "Why = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Sample input (sequence length = 3, batch size = 1, input size = 10)\n",
    "inputs = torch.randn(3, input_size)\n",
    "print(\"inputs:\",inputs, inputs.shape)\n",
    "\n",
    "# Define targets for classification\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "print(\"targets:\",targets, targets.shape)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h = torch.zeros(hidden_size, 1, requires_grad=True)  # Shape: [128, 1]\n",
    "c = torch.zeros(hidden_size, 1, requires_grad=True)  # Shape: [128, 1]\n",
    "print(\"h:\",h, h.shape)\n",
    "print(\"c:\",c, c.shape)\n",
    "print(\"inputsshape:\", inputs.shape)\n",
    "\n",
    "# Forward pass\n",
    "for i in range(inputs.size(0)):\n",
    "    input_t = inputs[i].unsqueeze(1)  # Add batch dimension for the first input in the sequence\n",
    "    print(\"*********\")\n",
    "    print(\"input_t\",{i},input_t, input_t.shape)\n",
    "    a1=torch.matmul(Wf, input_t)\n",
    "    print(\"a1:\",{i},a1, a1.shape)\n",
    "    print(\"Wf\",{i},Wf,Wf.shape)\n",
    "    a2=torch.matmul(Uf, h)\n",
    "    print(\"a2:\", {i}, a2, a2.shape)\n",
    "    print(\"Uf\", {i}, Uf, Uf.shape)\n",
    "    print(\"h\", {i}, h, h.shape)\n",
    "    print(\"bfor\", {i}, bf.unsqueeze)\n",
    "    print(\"bf\", {i}, bf.unsqueeze(1), bf.unsqueeze(1).shape)\n",
    "    a3=(torch.matmul(Wf, input_t) + torch.matmul(Uf, h) + bf.unsqueeze(1))  # Forget gate\n",
    "    print(\"a3:\", {i}, a3, a3.shape)\n",
    "    # Compute gate values\n",
    "    f = torch.sigmoid(a3)  # Forget gate\n",
    "    print(\"f\", {i}, f, f.shape)\n",
    "    a4=torch.matmul(Wi, input_t)\n",
    "    a5=torch.matmul(Ui, h)\n",
    "    a6=(torch.matmul(Wi, input_t) + torch.matmul(Ui, h) + bi.unsqueeze(1))\n",
    "  \n",
    "    i_gate = torch.sigmoid(a6)  # Input gate\n",
    "    print(\"Wi\",{i},Wi,Wi.shape)\n",
    "    print(\"Ui\", {i}, Ui, Ui.shape)\n",
    "    print(\"bi\", {i}, bi, bi.shape)\n",
    "    print(\"bi\", {i}, bi.unsqueeze(1), bi.unsqueeze(1).shape)\n",
    "    print(\"h\", {i}, h, h.shape)\n",
    "    print(\"i_gate\", {i}, i_gate, i_gate.shape)\n",
    "    o = torch.sigmoid(torch.matmul(Wo, input_t) + torch.matmul(Uo, h) + bo.unsqueeze(1))  # Output gate\n",
    "    c_hat = torch.tanh(torch.matmul(Wc, input_t) + torch.matmul(Uc, h) + bc.unsqueeze(1))  # Candidate cell state\n",
    "    \n",
    "    # Update cell and hidden states\n",
    "    c = f * c + i_gate * c_hat\n",
    "    h = o * torch.tanh(c)  # Update hidden state\n",
    "    print(\"h:\", h.shape)\n",
    "    print(\"c:\", c.shape)\n",
    "    print(\"f\", f.shape)\n",
    "    print(\"i\", i_gate.shape)\n",
    "    print(\"o\", o.shape)\n",
    "    print(\"c_hat\", c_hat.shape)\n",
    "\n",
    "# Compute output\n",
    "output = torch.matmul(Why, h.squeeze(1)) + by  # Reshape h to [hidden_size]\n",
    "output = output.squeeze(0)  # Ensure output is [output_size]\n",
    "\n",
    "# Compute loss (negative log likelihood loss)\n",
    "log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "loss = -log_probs[targets].sum()  # Summing the loss to ensure it is a scalar\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Print loss and gradients\n",
    "print('Loss:', loss.item())\n",
    "print('Gradients:')\n",
    "print('Wi.grad:', Wi.grad)\n",
    "print('Ui.grad:', Ui.grad)\n",
    "print('bi.grad:', bi.grad)\n",
    "print('Wf.grad:', Wf.grad)\n",
    "print('Uf.grad:', Uf.grad)\n",
    "print('bf.grad:', bf.grad)\n",
    "print('Wo.grad:', Wo.grad)\n",
    "print('Uo.grad:', Uo.grad)\n",
    "print('bo.grad:', bo.grad)\n",
    "print('Wc.grad:', Wc.grad)\n",
    "print('Uc.grad:', Uc.grad)\n",
    "print('bc.grad:', bc.grad)\n",
    "print('Why.grad:', Why.grad)\n",
    "print('by.grad:', by.grad)\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    Wi -= learning_rate * Wi.grad\n",
    "    Ui -= learning_rate * Ui.grad\n",
    "    bi -= learning_rate * bi.grad\n",
    "    Wf -= learning_rate * Wf.grad\n",
    "    Uf -= learning_rate * Uf.grad\n",
    "    bf -= learning_rate * bf.grad\n",
    "    Wo -= learning_rate * Wo.grad\n",
    "    Uo -= learning_rate * Uo.grad\n",
    "    bo -= learning_rate * bo.grad\n",
    "    Wc -= learning_rate * Wc.grad\n",
    "    Uc -= learning_rate * Uc.grad\n",
    "    bc -= learning_rate * bc.grad\n",
    "    Why -= learning_rate * Why.grad\n",
    "    by -= learning_rate * by.grad\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    Wi.grad.zero_()\n",
    "    Ui.grad.zero_()\n",
    "    bi.grad.zero_()\n",
    "    Wf.grad.zero_()\n",
    "    Uf.grad.zero_()\n",
    "    bf.grad.zero_()\n",
    "    Wo.grad.zero_()\n",
    "    Uo.grad.zero_()\n",
    "    bo.grad.zero_()\n",
    "    Wc.grad.zero_()\n",
    "    Uc.grad.zero_()\n",
    "    bc.grad.zero_()\n",
    "    Why.grad.zero_()\n",
    "    by.grad.zero_()\n",
    "\n",
    "print(\"predicted :\", output)\n",
    "softmax = torch.nn.functional.softmax(output, dim=0)\n",
    "print('Output (softmax probabilities):', softmax)\n",
    "print(\"target :\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Loss: 1.2108193238576253\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 1.2085041205088298\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 1.2062173287073772\n",
      "--------------------------------------------------\n",
      "Prediction for the new sentence: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wi = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Ui = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bi = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wf = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uf = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bf = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wo = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uo = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bo = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wc = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uc = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bc = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Why = torch.randn(self.output_size, self.hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(self.output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(self.hidden_size, 1, requires_grad=True)\n",
    "        c = torch.zeros(self.hidden_size, 1, requires_grad=True)\n",
    "        for t in range(x.size(0)):\n",
    "            input_t = x[t].unsqueeze(1)\n",
    "            f = torch.sigmoid(torch.matmul(self.Wf, input_t) + torch.matmul(self.Uf, h) + self.bf.unsqueeze(1))\n",
    "            i = torch.sigmoid(torch.matmul(self.Wi, input_t) + torch.matmul(self.Ui, h) + self.bi.unsqueeze(1))\n",
    "            o = torch.sigmoid(torch.matmul(self.Wo, input_t) + torch.matmul(self.Uo, h) + self.bo.unsqueeze(1))\n",
    "            c_hat = torch.tanh(torch.matmul(self.Wc, input_t) + torch.matmul(self.Uc, h) + self.bc.unsqueeze(1))\n",
    "            c = f * c + i * c_hat\n",
    "            h = o * torch.tanh(c)\n",
    "        output = torch.matmul(self.Why, h.squeeze(1)) + self.by\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.argmax(output, dim=0).item()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wi.grad is not None:\n",
    "            self.Wi.grad.zero_()\n",
    "        if self.Ui.grad is not None:\n",
    "            self.Ui.grad.zero_()\n",
    "        if self.bi.grad is not None:\n",
    "            self.bi.grad.zero_()\n",
    "        if self.Wf.grad is not None:\n",
    "            self.Wf.grad.zero_()\n",
    "        if self.Uf.grad is not None:\n",
    "            self.Uf.grad.zero_()\n",
    "        if self.bf.grad is not None:\n",
    "            self.bf.grad.zero_()\n",
    "        if self.Wo.grad is not None:\n",
    "            self.Wo.grad.zero_()\n",
    "        if self.Uo.grad is not None:\n",
    "            self.Uo.grad.zero_()\n",
    "        if self.bo.grad is not None:\n",
    "            self.bo.grad.zero_()\n",
    "        if self.Wc.grad is not None:\n",
    "            self.Wc.grad.zero_()\n",
    "        if self.Uc.grad is not None:\n",
    "            self.Uc.grad.zero_()\n",
    "        if self.bc.grad is not None:\n",
    "            self.bc.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wi, self.Ui, self.bi, self.Wf, self.Uf, self.bf, self.Wo, self.Uo, self.bo, self.Wc, self.Uc, self.bc, self.Why, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3  # Number of classes\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "print(\"encodedsentence:\",encoded_sentences)\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0, 1,2], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        print(\"input_seq:\",{i}, input_seq)\n",
    "        target = targets[i]\n",
    "        print(\"target:\", {i}, target)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (negative log likelihood loss)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "        loss = -log_probs[target]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting new sentence\n",
    "new_sentence = [\"my\", \"name\", \"is\", \"rithin\", \"a\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "print(f'Prediction for the new sentence: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Loss: 3.6251625219980874\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Average Loss: 4.167870998382568\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Average Loss: 0.5411695639292399\n",
      "--------------------------------------------------\n",
      "Binary Prediction for the new sentence: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "# Function to encode a string into a tensor\n",
    "def encode_string(s, input_size):\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(string.ascii_lowercase[:input_size])}\n",
    "    tensor = torch.zeros(input_size)\n",
    "    for char in s.lower():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx < input_size:\n",
    "                tensor[idx] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def encode_words(words, input_size, sequence_length):\n",
    "    encoded_words = [encode_string(word, input_size) for word in words]\n",
    "    if len(encoded_words) < sequence_length:\n",
    "        encoded_words.extend([torch.zeros(input_size)] * (sequence_length - len(encoded_words)))\n",
    "    elif len(encoded_words) > sequence_length:\n",
    "        encoded_words = encoded_words[:sequence_length]\n",
    "    return torch.stack(encoded_words)\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wi = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Ui = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bi = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wf = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uf = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bf = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wo = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uo = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bo = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wc = torch.randn(self.hidden_size, self.input_size, requires_grad=True)\n",
    "        self.Uc = torch.randn(self.hidden_size, self.hidden_size, requires_grad=True)\n",
    "        self.bc = torch.zeros(self.hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Why = torch.randn(self.output_size, self.hidden_size, requires_grad=True)\n",
    "        self.by = torch.zeros(self.output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(self.hidden_size, 1, requires_grad=True)\n",
    "        c = torch.zeros(self.hidden_size, 1, requires_grad=True)\n",
    "        for t in range(x.size(0)):\n",
    "            input_t = x[t].unsqueeze(1)\n",
    "            f = torch.sigmoid(torch.matmul(self.Wf, input_t) + torch.matmul(self.Uf, h) + self.bf.unsqueeze(1))\n",
    "            i = torch.sigmoid(torch.matmul(self.Wi, input_t) + torch.matmul(self.Ui, h) + self.bi.unsqueeze(1))\n",
    "            o = torch.sigmoid(torch.matmul(self.Wo, input_t) + torch.matmul(self.Uo, h) + self.bo.unsqueeze(1))\n",
    "            c_hat = torch.tanh(torch.matmul(self.Wc, input_t) + torch.matmul(self.Uc, h) + self.bc.unsqueeze(1))\n",
    "            c = f * c + i * c_hat\n",
    "            h = o * torch.tanh(c)\n",
    "        output = torch.matmul(self.Why, h.squeeze(1)) + self.by\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = self.forward(x)\n",
    "        return torch.sigmoid(output).item()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.Wi.grad is not None:\n",
    "            self.Wi.grad.zero_()\n",
    "        if self.Ui.grad is not None:\n",
    "            self.Ui.grad.zero_()\n",
    "        if self.bi.grad is not None:\n",
    "            self.bi.grad.zero_()\n",
    "        if self.Wf.grad is not None:\n",
    "            self.Wf.grad.zero_()\n",
    "        if self.Uf.grad is not None:\n",
    "            self.Uf.grad.zero_()\n",
    "        if self.bf.grad is not None:\n",
    "            self.bf.grad.zero_()\n",
    "        if self.Wo.grad is not None:\n",
    "            self.Wo.grad.zero_()\n",
    "        if self.Uo.grad is not None:\n",
    "            self.Uo.grad.zero_()\n",
    "        if self.bo.grad is not None:\n",
    "            self.bo.grad.zero_()\n",
    "        if self.Wc.grad is not None:\n",
    "            self.Wc.grad.zero_()\n",
    "        if self.Uc.grad is not None:\n",
    "            self.Uc.grad.zero_()\n",
    "        if self.bc.grad is not None:\n",
    "            self.bc.grad.zero_()\n",
    "        if self.Why.grad is not None:\n",
    "            self.Why.grad.zero_()\n",
    "        if self.by.grad is not None:\n",
    "            self.by.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.Wi, self.Ui, self.bi, self.Wf, self.Uf, self.bf, self.Wo, self.Uo, self.bo, self.Wc, self.Uc, self.bc, self.Why, self.by]\n",
    "\n",
    "# Define parameters\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1  # Binary classification output\n",
    "sequence_length = 5  # Define the sequence length\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    [\"my\", \"name\", \"is\", \"rithin\", \"a\"],\n",
    "    [\"i\", \"am\", \"a\", \"developer\", \"in\"],\n",
    "    [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [encode_words(sentence, input_size, sequence_length) for sentence in sentences]\n",
    "\n",
    "# Define targets for each sentence\n",
    "targets = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(encoded_sentences)):  # Iterate over each sentence\n",
    "        input_seq = encoded_sentences[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(input_seq)\n",
    "        \n",
    "        # Compute loss (binary cross-entropy loss with logits)\n",
    "        loss = loss_function(output, target.unsqueeze(0))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            model.zero_grad()  # Manually zero the gradients after updating weights\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f'Average Loss: {total_loss / len(encoded_sentences)}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Predicting new sentence\n",
    "new_sentence = [\"my\", \"name\", \"is\", \"rithin\", \"a\"]\n",
    "encoded_new_sentence = encode_words(new_sentence, input_size, sequence_length)\n",
    "prediction = model.predict(encoded_new_sentence)\n",
    "#print(f'Prediction for the new sentence: {prediction:.4f}')\n",
    "binary_prediction = 1 if prediction >= 0.5 else 0\n",
    "print(f'Binary Prediction for the new sentence: {binary_prediction}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputt tensor([[ 1.0750],\n",
      "        [-0.6875],\n",
      "        [ 0.6863],\n",
      "        [-1.2301],\n",
      "        [ 1.2975],\n",
      "        [ 0.3874],\n",
      "        [ 0.3056],\n",
      "        [ 0.8377],\n",
      "        [ 0.5743],\n",
      "        [-0.5956]])\n",
      "inputt tensor([[-0.0610],\n",
      "        [-0.2946],\n",
      "        [-1.3526],\n",
      "        [ 0.5550],\n",
      "        [-2.1487],\n",
      "        [-0.2415],\n",
      "        [ 0.3168],\n",
      "        [-0.0286],\n",
      "        [ 0.9466],\n",
      "        [ 0.4472]])\n",
      "inputt tensor([[ 1.0562],\n",
      "        [ 0.1735],\n",
      "        [-0.6131],\n",
      "        [-0.8838],\n",
      "        [-1.8751],\n",
      "        [ 1.9588],\n",
      "        [ 0.3802],\n",
      "        [ 0.5827],\n",
      "        [-0.3728],\n",
      "        [ 0.1304]])\n",
      "rinputt tensor([[ 1.0562],\n",
      "        [ 0.1735],\n",
      "        [-0.6131],\n",
      "        [-0.8838],\n",
      "        [-1.8751],\n",
      "        [ 1.9588],\n",
      "        [ 0.3802],\n",
      "        [ 0.5827],\n",
      "        [-0.3728],\n",
      "        [ 0.1304]])\n",
      "rinputt tensor([[-0.0610],\n",
      "        [-0.2946],\n",
      "        [-1.3526],\n",
      "        [ 0.5550],\n",
      "        [-2.1487],\n",
      "        [-0.2415],\n",
      "        [ 0.3168],\n",
      "        [-0.0286],\n",
      "        [ 0.9466],\n",
      "        [ 0.4472]])\n",
      "rinputt tensor([[ 1.0750],\n",
      "        [-0.6875],\n",
      "        [ 0.6863],\n",
      "        [-1.2301],\n",
      "        [ 1.2975],\n",
      "        [ 0.3874],\n",
      "        [ 0.3056],\n",
      "        [ 0.8377],\n",
      "        [ 0.5743],\n",
      "        [-0.5956]])\n",
      "Loss: 20.508708953857422\n",
      "Gradients:\n",
      "Wi_f.grad: tensor([[-1.5195,  0.5359, -0.4701,  ..., -1.0674, -0.2765,  0.5359],\n",
      "        [ 0.2986, -0.1894,  0.1832,  ...,  0.2320,  0.1595, -0.1622],\n",
      "        [ 0.0330,  0.0390,  0.2652,  ...,  0.0226, -0.1613, -0.0956],\n",
      "        ...,\n",
      "        [ 0.0701,  0.0098, -0.0465,  ...,  0.0388, -0.0200,  0.0104],\n",
      "        [-0.5296,  0.3460, -0.3097,  ..., -0.4131, -0.3042,  0.2845],\n",
      "        [ 1.0983, -0.6832,  0.7486,  ...,  0.8536,  0.5398, -0.6216]])\n",
      "Ui_f.grad: tensor([[ 1.1649e-02, -2.8972e-02,  3.0188e-03,  ..., -2.3256e-04,\n",
      "          2.6292e-01,  4.1118e-02],\n",
      "        [ 1.8629e-03, -1.0217e-03, -5.6575e-04,  ..., -2.9335e-05,\n",
      "         -1.6769e-03, -2.8028e-05],\n",
      "        [-1.0780e-01,  6.6768e-02,  3.0517e-02,  ...,  1.7141e-03,\n",
      "          4.4700e-03, -1.2359e-02],\n",
      "        ...,\n",
      "        [ 8.2551e-03, -2.1141e-03, -3.2076e-03,  ..., -1.2474e-04,\n",
      "         -3.6650e-02, -4.5372e-03],\n",
      "        [-1.2649e-02,  7.8445e-03,  3.5779e-03,  ...,  2.0115e-04,\n",
      "          4.0208e-04, -1.4686e-03],\n",
      "        [-2.4102e-02,  1.5221e-02,  6.7383e-03,  ...,  3.8388e-04,\n",
      "         -2.5382e-03, -3.2975e-03]])\n",
      "bi_f.grad: tensor([-1.3283e+00,  2.8069e-01, -1.6399e-01,  5.7844e-01, -3.4673e-02,\n",
      "        -1.0826e-03,  2.6496e-01, -2.4599e-01,  2.0254e-02, -1.4700e-01,\n",
      "        -5.1637e-02, -3.5448e-01, -1.8272e-01,  8.4831e-02, -5.2962e-01,\n",
      "        -6.6024e-02,  6.5294e-01, -2.4810e+00,  1.0639e+00, -7.1904e-03,\n",
      "         5.5845e-01,  6.4879e-02, -7.2603e-03,  1.5183e+00,  2.6247e-02,\n",
      "         1.0081e+00, -7.0130e-01, -1.0725e-01,  6.6576e-01,  2.3305e-01,\n",
      "         3.3456e-01,  1.3306e-01,  2.6417e-01,  1.6835e-02, -5.6120e-02,\n",
      "        -1.4025e-01,  3.0464e-02,  3.2283e-01, -1.4407e-01,  3.1922e-02,\n",
      "        -6.3500e-01,  4.7795e-01,  1.0088e-01,  7.3655e-02, -8.3918e-01,\n",
      "        -1.4138e-01, -1.1648e+00, -4.3956e-01,  5.4649e-01, -1.1527e-01,\n",
      "         4.7503e-04, -1.1437e+00,  1.1355e+00, -5.9972e-01, -3.7464e-03,\n",
      "        -4.7401e-01,  1.0346e+00,  1.8890e+00, -2.5396e-01, -2.7011e-02,\n",
      "         5.7128e-03, -1.6933e-02, -4.6952e-01,  2.2312e-04,  3.3806e-01,\n",
      "         1.7899e-01, -1.8028e-01, -6.6092e-02,  1.1032e-01,  7.3439e-03,\n",
      "        -3.1234e-01, -4.0491e-01, -7.5661e-01, -2.2857e-01,  1.6202e-02,\n",
      "        -7.5538e-02, -1.8814e+00,  3.3128e-01,  6.3658e-01, -1.6012e+00,\n",
      "        -7.9499e-01,  4.1128e-01, -2.5303e-01,  9.4183e-01, -6.5007e-03,\n",
      "         8.7638e-01, -1.0140e+00,  1.5612e-01,  9.3577e-02,  7.0726e-02,\n",
      "         7.5431e-03,  5.3615e-03,  4.5667e-01, -9.8191e-03,  2.2745e-01,\n",
      "         1.0384e+00,  1.1445e+00, -2.8036e-02,  5.5198e-01,  2.2683e-02,\n",
      "        -1.0589e-01, -1.0495e-02, -1.0634e-01,  1.1590e+00, -7.8192e-01,\n",
      "        -1.6874e-02, -1.0851e+00,  6.3029e-02,  1.5955e+00, -8.7357e-02,\n",
      "        -1.0108e-01,  1.2543e-02, -4.9090e-02,  7.9667e-01, -2.5501e-01,\n",
      "        -2.5138e-01,  3.5318e-01, -1.9148e+00, -2.2948e-02, -1.1090e-01,\n",
      "        -3.4572e-01, -4.3904e-01,  1.0157e+00,  6.0041e-03,  4.0875e-01,\n",
      "         7.1243e-02, -5.1553e-01,  9.7725e-01])\n",
      "Wf_f.grad: tensor([[-8.1247e-02,  2.5628e-02,  2.3722e-01,  ..., -4.5519e-02,\n",
      "         -9.7985e-02, -7.2298e-02],\n",
      "        [ 3.0646e-03,  1.9603e-03,  5.3255e-03,  ...,  1.6650e-03,\n",
      "         -5.8168e-03, -1.9494e-03],\n",
      "        [-2.9163e-02, -1.4214e-01, -6.5285e-01,  ..., -1.3650e-02,\n",
      "          4.5669e-01,  2.1585e-01],\n",
      "        ...,\n",
      "        [ 1.9826e-04,  3.1471e-03,  1.5073e-02,  ...,  5.4058e-05,\n",
      "         -1.0192e-02, -4.9517e-03],\n",
      "        [-1.1030e-02, -1.2308e-03,  9.2353e-03,  ..., -6.0959e-03,\n",
      "          2.0058e-03, -2.2896e-03],\n",
      "        [ 8.2840e-04, -6.6004e-04, -4.3631e-03,  ...,  4.7120e-04,\n",
      "          2.2949e-03,  1.3742e-03]])\n",
      "Uf_f.grad: tensor([[-8.7116e-02,  5.0162e-02,  2.5764e-02,  ...,  1.3770e-03,\n",
      "          4.9568e-02, -3.0467e-03],\n",
      "        [-2.7728e-03,  1.8377e-03,  7.5005e-04,  ...,  4.4351e-05,\n",
      "         -1.3409e-03, -5.3777e-04],\n",
      "        [ 2.8200e-01, -1.7484e-01, -7.9782e-02,  ..., -4.4844e-03,\n",
      "         -9.5504e-03,  3.2654e-02],\n",
      "        ...,\n",
      "        [-6.4314e-03,  3.9676e-03,  1.8253e-03,  ...,  1.0223e-04,\n",
      "          4.5893e-04, -7.0829e-04],\n",
      "        [-2.0768e-03,  8.0789e-04,  7.2683e-04,  ...,  3.1982e-05,\n",
      "          5.8783e-03,  6.3672e-04],\n",
      "        [ 1.7136e-03, -1.0195e-03, -4.9726e-04,  ..., -2.7157e-05,\n",
      "         -5.7744e-04,  1.1998e-04]])\n",
      "bf_f.grad: tensor([-2.2176e-01, -2.5125e-03,  4.8282e-01,  4.6116e-01, -6.9659e-04,\n",
      "        -5.1162e-01,  1.0696e-03, -1.3291e-01,  4.6113e-01, -5.4382e-02,\n",
      "        -1.4234e+00, -1.1015e-01,  4.5753e-02, -1.0052e-02, -2.5264e-03,\n",
      "         3.9742e-04, -1.1136e-04,  7.6738e-04,  7.1064e-02, -6.6757e-02,\n",
      "        -7.9593e-04, -5.6382e-01,  2.4570e-01, -8.9483e-02, -9.5288e-04,\n",
      "         1.5265e-02, -3.1135e-03, -1.2845e-02,  3.0764e-04,  3.2576e-02,\n",
      "         6.0773e-04, -1.1379e-01, -6.7881e-02,  9.4935e-05, -1.0999e-02,\n",
      "        -6.5268e-01, -1.1317e-03,  7.9224e-03, -9.5450e-02,  9.5877e-02,\n",
      "        -2.3655e-02, -2.0359e-02,  1.5149e-03, -1.2317e-02, -3.3439e-02,\n",
      "        -3.5813e-01,  2.7304e-02,  1.1298e-01, -1.1085e-03,  3.2947e-02,\n",
      "         8.8429e-03, -3.3087e-01, -8.8047e-01, -1.8495e-01,  2.4426e-05,\n",
      "         5.0288e-03,  9.6878e-03, -2.1210e-03,  2.2220e-02,  1.1526e-02,\n",
      "        -3.3637e-03,  8.6185e-02,  1.9364e-03,  8.7944e-02,  6.0686e-02,\n",
      "        -2.7881e-02,  1.9447e-03,  1.6044e-01, -1.5866e-02,  5.7989e-03,\n",
      "         2.8421e-03,  9.5796e-03, -2.3230e-03,  1.2977e-02,  4.0224e-03,\n",
      "         5.6123e-02,  3.7431e-03,  9.2177e-02,  2.0487e-02,  6.4306e-03,\n",
      "         1.7034e-01, -3.9537e-01,  2.3450e-03, -3.8808e-04, -5.7276e-03,\n",
      "         1.1782e-01,  1.8755e-02, -2.1053e-01,  2.3608e-02,  4.9616e-02,\n",
      "         1.4085e-01,  7.0646e-03, -5.0518e-01, -5.9514e-04, -7.9259e-03,\n",
      "         5.5501e-02,  1.4621e-02, -4.1128e-02,  1.1827e-03, -1.3898e-02,\n",
      "         2.1670e-01,  7.5300e-05,  1.0213e-01,  1.9943e-01,  1.7763e-01,\n",
      "        -5.3503e-06,  3.5558e-02, -2.3109e-01, -2.2010e-02, -1.2823e-02,\n",
      "         1.1435e-01,  3.1763e-05,  1.3726e-01,  3.5796e-02, -5.8530e-02,\n",
      "         1.6805e-01,  1.1451e-01, -7.3022e-03, -4.4352e-06,  4.7411e-04,\n",
      "        -8.2435e-04,  5.4354e-03,  4.2499e-02, -9.0405e-02,  4.8483e-03,\n",
      "        -1.1387e-02, -1.2602e-02,  3.7429e-03])\n",
      "Wo_f.grad: tensor([[-0.2077,  0.1319, -0.1341,  ..., -0.1617, -0.1090,  0.1154],\n",
      "        [ 0.3117, -0.1013,  0.1560,  ...,  0.2195,  0.0228, -0.1304],\n",
      "        [ 0.2860, -0.2554,  0.1730,  ...,  0.2384,  0.2729, -0.1739],\n",
      "        ...,\n",
      "        [ 0.0437, -0.0233,  0.0231,  ...,  0.0328,  0.0174, -0.0211],\n",
      "        [ 0.0619,  0.1442,  0.3323,  ...,  0.0199, -0.3617, -0.0975],\n",
      "        [ 0.0182,  0.0148,  0.1155,  ...,  0.0127, -0.0677, -0.0428]])\n",
      "Uo_f.grad: tensor([[ 8.9288e-04, -5.7502e-04, -2.4638e-04,  ..., -1.4245e-05,\n",
      "          2.2923e-04,  1.4258e-04],\n",
      "        [-2.6186e-02,  2.0445e-02,  6.1861e-03,  ...,  4.2557e-04,\n",
      "         -5.0081e-02, -1.0730e-02],\n",
      "        [ 3.3387e-02, -2.3408e-02, -8.6594e-03,  ..., -5.3682e-04,\n",
      "          3.1653e-02,  8.8174e-03],\n",
      "        ...,\n",
      "        [-3.0561e-04,  4.1801e-04,  2.0113e-05,  ...,  5.3571e-06,\n",
      "         -2.7564e-03, -4.5326e-04],\n",
      "        [-1.7574e-01,  1.1319e-01,  4.8489e-02,  ...,  2.8038e-03,\n",
      "         -4.5308e-02, -2.8091e-02],\n",
      "        [-4.6228e-02,  2.8663e-02,  1.3078e-02,  ...,  7.3512e-04,\n",
      "          1.5417e-03, -5.3565e-03]])\n",
      "bo_f.grad: tensor([-1.9154e-01,  2.3016e-01,  3.3437e-01,  1.9204e+00, -3.8862e-01,\n",
      "        -1.8775e+00,  4.9930e-02, -2.4045e-01,  6.8908e-01, -1.7637e+00,\n",
      "        -8.5132e-01, -1.3411e-01, -6.2008e-01, -3.6091e-02, -1.9452e+00,\n",
      "        -1.5895e+00,  1.9527e-02,  1.1230e+00,  1.9825e-02, -1.0531e-02,\n",
      "         2.6380e-01, -3.3646e-02, -1.1228e+00, -4.2431e-01,  2.9174e-01,\n",
      "         2.0272e+00, -2.5094e-01,  4.6096e-01,  6.6329e-01, -2.6817e-01,\n",
      "        -1.5103e-01,  2.1623e-01,  3.2610e-01,  2.1941e-02, -2.8423e+00,\n",
      "         4.8595e-01,  9.3328e-01,  3.8118e-01, -1.0124e+00,  3.5099e-03,\n",
      "         1.0425e+00,  6.7387e-01,  3.7950e-01, -1.3266e+00,  3.0064e-01,\n",
      "        -4.4949e-01, -2.6134e-01, -1.8159e-01,  1.9825e-01, -1.2511e+00,\n",
      "         4.2382e-02, -7.1868e-02, -2.1319e+00, -1.7414e+00, -1.0581e-02,\n",
      "        -8.6904e-01, -5.9646e-01,  4.3079e-01, -3.1041e-01,  9.9393e-02,\n",
      "         6.4717e-02,  7.9596e-01, -2.3362e-01,  1.3815e+00,  6.1467e-01,\n",
      "         5.1607e-01,  1.7539e-01,  9.7158e-02,  7.7623e-02,  2.6036e-01,\n",
      "         6.8433e-02, -1.9580e-01, -7.6114e-01,  1.3276e-02,  2.6944e-03,\n",
      "        -1.5394e+00, -8.9603e-01,  1.1723e+00, -4.4833e-02, -3.8007e-01,\n",
      "        -4.7671e-01, -7.9427e-01, -4.9025e-02, -2.1403e-02, -5.9576e-02,\n",
      "         7.2987e-01, -2.1596e+00, -1.5765e-02,  5.8078e-01,  3.2889e+00,\n",
      "         1.9594e-01, -1.6251e-01,  2.3682e+00,  4.6300e-02,  3.4616e-01,\n",
      "        -4.4000e-01,  1.5827e-01,  4.3877e-01, -1.2930e-01,  3.7860e-01,\n",
      "        -1.5534e-01, -1.7704e-01, -9.8934e-01,  1.1726e+00,  3.1467e-01,\n",
      "        -6.2201e-03, -5.1141e-01,  6.4695e-01,  1.6033e+00, -3.9777e-01,\n",
      "        -7.3070e-02,  1.3040e-02, -1.2288e-01,  2.7707e+00, -3.2898e-01,\n",
      "        -1.1994e-01, -9.9528e-01, -1.1875e-01, -4.8748e-04, -6.2072e-01,\n",
      "        -2.7445e+00,  2.8672e-02, -2.0045e-01, -1.5314e-01, -4.8548e-01,\n",
      "         3.9437e-02, -2.7273e-01, -6.6672e-02])\n",
      "Wc_f.grad: tensor([[-0.1121,  0.0815, -0.0323,  ..., -0.0879, -0.0890,  0.0497],\n",
      "        [-0.0761,  0.0432, -0.0697,  ..., -0.0590, -0.0248,  0.0488],\n",
      "        [-0.0014,  0.0009, -0.0009,  ..., -0.0011, -0.0008,  0.0008],\n",
      "        ...,\n",
      "        [-0.0150,  0.0099, -0.0100,  ..., -0.0118, -0.0083,  0.0086],\n",
      "        [ 0.6395, -0.3862,  0.3731,  ...,  0.4919,  0.3167, -0.3349],\n",
      "        [ 0.0036, -0.0026,  0.0027,  ...,  0.0029,  0.0022, -0.0022]])\n",
      "Uc_f.grad: tensor([[-1.7405e-02,  1.0788e-02,  4.9249e-03,  ...,  2.7676e-04,\n",
      "          6.2606e-04, -2.0098e-03],\n",
      "        [ 9.4408e-03, -5.8563e-03, -2.6700e-03,  ..., -1.5013e-04,\n",
      "         -2.8382e-04,  1.0986e-03],\n",
      "        [-3.7814e-10,  2.9305e-11,  1.6654e-10,  ...,  5.5668e-12,\n",
      "          2.4964e-09,  3.3132e-10],\n",
      "        ...,\n",
      "        [ 3.6323e-05, -6.1181e-06, -1.5038e-05,  ..., -5.4191e-07,\n",
      "         -1.9981e-04, -2.5786e-05],\n",
      "        [ 2.4565e-03, -2.9978e-04, -1.0501e-03,  ..., -3.6402e-05,\n",
      "         -1.4893e-02, -1.9523e-03],\n",
      "        [-2.8830e-05,  2.2343e-06,  1.2697e-05,  ...,  4.2443e-07,\n",
      "          1.9034e-04,  2.5261e-05]])\n",
      "bc_f.grad: tensor([-1.3573e-01, -5.3681e-02, -1.3410e-03, -1.6388e-01, -5.3048e-03,\n",
      "         1.2920e-02,  2.7090e+00,  5.8430e-01, -1.0582e-01,  1.3224e+00,\n",
      "        -2.1680e-02,  3.9602e+00, -1.8662e-03, -7.0906e-05,  4.1177e-01,\n",
      "        -5.5876e-01,  3.6757e-02,  1.7080e-01,  5.0984e-03,  6.1731e-02,\n",
      "         1.0633e-02, -1.3432e-01,  3.1460e+00, -7.0393e-01,  4.1172e-03,\n",
      "        -1.6774e+00,  2.0169e-01,  4.7564e-02,  4.4354e-02, -1.0877e-02,\n",
      "         2.1976e-02, -5.2768e-02, -8.4303e-05, -4.1019e-01, -2.5886e+00,\n",
      "         8.0077e-03, -2.1475e+00, -1.2490e-01, -2.1996e+00,  9.1544e-01,\n",
      "         3.6545e-02, -7.4716e-02,  3.4660e-03,  1.8196e-02, -5.0485e-02,\n",
      "         4.6420e-02, -2.7166e+00, -1.4378e+01, -1.0016e+01, -6.4837e-01,\n",
      "        -1.7648e+00,  7.2615e-01, -7.2846e-03, -1.4450e+00, -1.0360e-03,\n",
      "         8.3253e-01, -3.3951e-02, -7.7381e+00, -2.2660e-01,  4.1209e-01,\n",
      "         1.3211e-02,  6.4036e-01, -1.7983e-05, -1.0897e+00, -1.1967e-01,\n",
      "         1.0201e-01, -2.4879e-01,  9.1822e-02,  7.0956e+00,  1.0389e-01,\n",
      "         1.5547e-01,  1.4274e-01,  1.8440e-01,  3.9425e-02,  7.7441e-02,\n",
      "        -2.4460e-03,  6.6105e-02,  1.1625e-01, -2.6586e-01,  2.4557e+00,\n",
      "        -1.3102e-01, -2.2057e+00,  2.7965e-01,  1.1841e+00, -2.3772e-03,\n",
      "        -6.3144e-03, -1.7922e-02, -3.0550e-02,  2.1993e-02, -3.4277e-01,\n",
      "        -1.2330e+00, -1.6456e-01, -5.8199e-01,  1.5428e-02, -1.3573e-01,\n",
      "        -5.9156e+00,  3.1934e-01, -2.3538e-01,  4.6742e-02, -8.1977e-01,\n",
      "        -7.6275e-01,  1.1776e+00,  1.0835e-02, -1.2007e-01, -2.7650e-01,\n",
      "         2.1819e-03,  1.7245e-01, -3.5143e-01,  1.8838e-01, -2.4417e+00,\n",
      "        -7.8913e-02, -9.5808e-03,  9.7156e-03,  3.8418e+00,  1.3250e-01,\n",
      "         4.8780e-01, -1.2456e+00,  6.5114e-02,  7.0616e-03, -1.1607e+01,\n",
      "        -1.8088e-01, -4.5402e-03, -3.2948e-01, -2.5271e-03, -3.8110e-03,\n",
      "        -1.3908e-02,  5.9567e-01,  3.3459e-03])\n",
      "Wi_b.grad: tensor([[-0.0119, -0.0020,  0.0069,  ..., -0.0066,  0.0042, -0.0015],\n",
      "        [-0.0012, -0.0002,  0.0007,  ..., -0.0007,  0.0004, -0.0002],\n",
      "        [ 0.0081,  0.0013, -0.0047,  ...,  0.0045, -0.0029,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0411,  0.0067, -0.0238,  ...,  0.0227, -0.0145,  0.0051],\n",
      "        [ 0.0056,  0.0009, -0.0033,  ...,  0.0031, -0.0020,  0.0007],\n",
      "        [-0.0012, -0.0002,  0.0007,  ..., -0.0006,  0.0004, -0.0001]])\n",
      "Ui_b.grad: tensor([[-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., 0., 0.]])\n",
      "bi_b.grad: tensor([-1.1308e-02, -1.1605e-03,  7.7130e-03,  3.8831e-01, -3.2491e-02,\n",
      "         3.7462e-01, -8.1094e-03, -8.8530e-04,  8.0521e-03, -1.4815e-04,\n",
      "         1.0458e-01, -2.8338e-02,  4.7153e-02, -2.2636e-01,  9.1250e-03,\n",
      "        -5.6235e-02, -8.7565e-02,  1.6394e-02,  2.1359e-01,  1.2511e-01,\n",
      "         5.6255e-07, -1.2096e-02,  2.2938e-02,  5.6121e-01,  1.0386e+00,\n",
      "         5.6873e-04, -3.3987e-03, -6.0483e-04, -7.7273e-04, -3.0579e-05,\n",
      "         2.4382e-01,  2.7262e-03,  6.0821e-03, -6.4744e-05, -1.5422e-01,\n",
      "         9.6554e-02, -1.6107e-02,  2.9017e-02,  3.8403e-02, -1.4809e-03,\n",
      "         1.0291e-01,  4.0561e-02,  1.2055e-01, -1.3254e-03, -3.6799e-02,\n",
      "        -2.1058e-03,  9.2615e-02,  9.8552e-04, -1.1726e-04, -1.0494e-01,\n",
      "        -7.9625e-02, -3.4709e-02,  1.7921e-02, -1.2221e-02, -1.1182e-01,\n",
      "        -3.8920e-01,  1.9534e-01, -8.1106e-02,  1.4391e-02,  1.2443e-02,\n",
      "        -1.7222e-03,  1.0685e-03, -1.6410e-03,  2.0589e-02,  8.6655e-03,\n",
      "        -5.8950e-01,  4.4326e-02,  4.9073e-01, -2.1485e-01,  1.2328e-01,\n",
      "         1.8609e-04, -4.5651e-03, -2.4725e-03,  2.4135e-01, -3.7403e-03,\n",
      "         5.6867e-02,  1.7974e-02,  1.7232e-02, -1.0928e-01,  8.5959e-03,\n",
      "         5.5203e-02,  9.9546e-01, -8.2208e-01, -9.0780e-02, -8.1162e-02,\n",
      "        -5.4355e-01, -1.1618e-04, -1.2252e-01, -3.4085e-06,  1.0255e-02,\n",
      "        -1.0385e-03,  9.8284e-04,  9.9354e-02,  1.0711e-01, -2.7351e-04,\n",
      "         2.0890e-04,  7.3542e-01, -6.5794e-01,  1.9860e-02, -2.3623e-01,\n",
      "        -1.0218e-03,  5.1105e-02, -8.6367e-01,  1.6399e-01, -1.2950e-03,\n",
      "         8.3592e-03,  3.1462e-02, -9.0374e-01,  4.1917e-01, -8.8951e-02,\n",
      "        -1.4067e-03,  1.2895e-02, -1.2825e-01,  1.8972e-03, -6.9112e-03,\n",
      "        -2.9219e-03, -2.1743e-01, -5.4498e-02,  9.6535e-03, -9.3509e-02,\n",
      "        -2.1165e-03, -1.9629e-02,  1.1997e-02,  2.4910e-02,  1.2267e-02,\n",
      "         3.8874e-02,  5.3203e-03, -1.1054e-03])\n",
      "Wf_b.grad: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., -0., 0., 0.]])\n",
      "Uf_b.grad: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "bf_b.grad: tensor([0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., 0., -0., -0.,\n",
      "        -0., 0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., 0., 0.,\n",
      "        -0., 0., -0., 0., 0., -0., 0., 0., 0., -0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., -0., 0.])\n",
      "Wo_b.grad: tensor([[-0.0204, -0.0034,  0.0119,  ..., -0.0113,  0.0072, -0.0025],\n",
      "        [-0.0696, -0.0114,  0.0404,  ..., -0.0384,  0.0246, -0.0086],\n",
      "        [ 0.1100,  0.0181, -0.0639,  ...,  0.0607, -0.0388,  0.0136],\n",
      "        ...,\n",
      "        [ 0.0526,  0.0086, -0.0305,  ...,  0.0290, -0.0186,  0.0065],\n",
      "        [ 0.0054,  0.0009, -0.0031,  ...,  0.0030, -0.0019,  0.0007],\n",
      "        [-0.0813, -0.0133,  0.0472,  ..., -0.0448,  0.0287, -0.0100]])\n",
      "Uo_b.grad: tensor([[-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., 0., 0.]])\n",
      "bo_b.grad: tensor([-1.9358e-02, -6.5945e-02,  1.0418e-01,  2.1691e-02, -5.6325e-02,\n",
      "         8.9536e-01, -2.0479e-01, -8.5748e-04,  1.1310e+00, -1.9014e-04,\n",
      "         1.1983e-02, -7.7972e-02,  7.5620e-03, -1.2924e-02,  1.0714e-02,\n",
      "        -4.2458e-01, -2.9439e-02,  4.4639e-03,  1.9776e-01,  3.5191e-01,\n",
      "         9.7735e-05, -7.2379e-01,  1.7107e-01,  6.1398e-01,  1.9251e-01,\n",
      "         5.8833e-04, -2.7149e-01, -6.5215e-04, -2.9222e-01, -1.8015e-03,\n",
      "         7.8803e-02,  2.7401e-03,  8.0010e-03, -1.7559e-02, -3.7804e-02,\n",
      "         2.8161e-03, -1.8386e-02,  2.5918e-02,  1.7379e-03, -7.8173e-01,\n",
      "         2.5439e-01,  9.5656e-02,  8.5898e-02, -4.5886e-01, -7.5480e-02,\n",
      "        -1.0794e-02,  1.8089e-03,  4.6507e-03, -1.1397e-04, -1.3193e-01,\n",
      "        -1.5138e-01, -3.8654e-02,  1.6752e-02, -1.2810e-01, -3.6357e-01,\n",
      "        -1.8457e-01,  4.4288e-03, -3.4410e-03,  1.7189e-01,  4.7994e-03,\n",
      "        -7.1590e-03,  1.4241e-03, -4.4686e-03,  2.8173e-02,  2.1140e-01,\n",
      "        -2.1418e-01,  2.0044e-02,  2.6654e-01, -1.3541e-02,  1.0788e-01,\n",
      "         2.0805e-04, -2.0880e-03, -1.3001e-01,  1.2891e-01, -7.5988e-02,\n",
      "         2.6322e-01,  2.2321e-02,  2.6884e-02, -2.8191e-02,  5.6146e-04,\n",
      "         1.7041e-01,  3.9690e-01, -3.8199e-01, -1.7205e-01, -3.3777e-01,\n",
      "        -5.1184e-03, -6.4578e-03, -1.2480e-01, -3.4687e-03,  9.3307e-03,\n",
      "        -3.3901e-01,  1.0253e-03,  1.2838e-02,  9.5463e-02, -2.7446e-04,\n",
      "         3.0763e-01,  8.2544e-03, -9.1538e-02,  1.2483e-02, -7.0467e-01,\n",
      "        -8.9418e-03,  2.9324e-02, -1.4911e-01,  1.5657e-01, -7.5397e-06,\n",
      "         4.9382e-01,  1.9159e-01, -1.0319e-01,  1.0657e+00, -7.9770e-02,\n",
      "        -4.0964e-03,  1.4230e-02, -1.1181e-01,  2.4746e-04, -5.0163e-01,\n",
      "        -2.1207e-01, -6.0806e-02, -7.3240e-03,  9.5999e-05, -6.0421e-01,\n",
      "        -3.5283e-02, -1.5207e-02,  8.9977e-04,  3.0657e-02,  3.2156e-04,\n",
      "         4.9761e-02,  5.0927e-03, -7.6941e-02])\n",
      "Wc_b.grad: tensor([[ 5.5449e-03,  9.1075e-04, -3.2189e-03,  ...,  3.0593e-03,\n",
      "         -1.9573e-03,  6.8450e-04],\n",
      "        [-9.3162e-04, -1.5302e-04,  5.4081e-04,  ..., -5.1400e-04,\n",
      "          3.2885e-04, -1.1500e-04],\n",
      "        [ 1.1625e-01,  1.9093e-02, -6.7482e-02,  ...,  6.4137e-02,\n",
      "         -4.1034e-02,  1.4350e-02],\n",
      "        ...,\n",
      "        [ 1.1365e-02,  1.8667e-03, -6.5976e-03,  ...,  6.2706e-03,\n",
      "         -4.0119e-03,  1.4030e-03],\n",
      "        [-3.0356e-05, -4.9859e-06,  1.7622e-05,  ..., -1.6748e-05,\n",
      "          1.0715e-05, -3.7473e-06],\n",
      "        [-1.3849e-01, -2.2746e-02,  8.0392e-02,  ..., -7.6407e-02,\n",
      "          4.8884e-02, -1.7095e-02]])\n",
      "Uc_b.grad: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., 0.]])\n",
      "bc_b.grad: tensor([ 5.2501e-03, -8.8209e-04,  1.1007e-01,  9.3063e-06, -5.7195e-01,\n",
      "        -3.1863e-02,  2.6509e-04, -6.3957e-06, -8.3119e-03, -4.9729e-07,\n",
      "         2.6674e-03, -3.2941e-03,  2.3465e-03,  5.9767e-03, -3.1813e-03,\n",
      "         3.3550e-05, -2.9156e-02,  1.0525e-04,  3.7470e-02, -1.3090e-04,\n",
      "        -1.0984e-04,  2.6006e-04, -2.6053e-03, -1.8653e-02, -7.6099e-04,\n",
      "         9.6470e-04, -1.3604e+00,  5.3159e-02, -4.2594e-03,  3.3688e-06,\n",
      "        -1.0190e-05,  2.4135e-04,  1.7599e-03, -1.7136e-01,  9.6208e-02,\n",
      "         2.1770e-06, -9.2437e-02,  2.4297e-02,  5.6594e-02, -2.9881e-02,\n",
      "         9.9408e-02, -8.1948e-01, -3.6229e-01,  2.9873e-02, -1.8049e-02,\n",
      "         2.3997e-02,  4.7960e-03,  3.1605e-01,  4.2781e-06, -2.0063e-01,\n",
      "         1.5340e-03, -4.4137e+00,  8.1523e-03, -8.6999e-01,  3.7071e-01,\n",
      "         3.0889e-02, -3.9332e-02, -3.4020e+00, -3.4105e-01,  1.2060e-04,\n",
      "         5.7433e-05,  3.4044e-04, -9.9090e-06,  4.1286e-03, -8.3330e-04,\n",
      "         6.6044e-02, -8.7515e-06,  1.9511e-03,  7.7910e-04, -3.6015e-03,\n",
      "         1.3294e-03,  5.8143e-04,  3.3516e-02, -2.5321e-01, -9.4517e-01,\n",
      "        -8.4210e-06,  2.3385e-02,  6.7998e-03,  4.8843e-01, -2.8200e-01,\n",
      "        -4.6035e-01, -1.0241e-01, -1.6726e-01, -3.7925e-08,  1.5447e+00,\n",
      "         4.5064e-02,  5.3966e-01, -1.6643e-02,  1.1956e-05,  2.0023e-05,\n",
      "         4.1702e-01,  7.4393e-06, -2.2769e-03, -9.7356e-03, -2.2241e-08,\n",
      "         4.0289e-07, -1.1798e+00,  1.7300e-02, -3.5076e-04,  1.8429e-06,\n",
      "        -1.7931e-05, -2.5143e-04, -2.5134e-01,  1.7006e-01,  3.8337e-05,\n",
      "        -9.8089e-03, -8.0331e-03,  1.5572e-03,  2.1563e-04, -1.4327e-03,\n",
      "        -5.2612e-03,  4.1764e-07,  4.6616e-02, -2.5815e-05, -5.2541e-03,\n",
      "         9.6520e-03,  3.1529e-02, -7.4401e-02,  8.7965e-04, -5.4794e-04,\n",
      "        -8.8717e-03, -2.1941e-04,  7.0007e-02,  1.9143e-03,  6.9867e-06,\n",
      "         1.0761e-02, -2.8742e-05, -1.3112e-01])\n",
      "Why.grad: tensor([[ 4.8325e-01, -6.2576e-02,  3.7156e-02,  1.1794e+00,  1.5055e+00,\n",
      "         -1.1444e-04,  1.2296e-01, -3.0717e-02, -2.6031e+00, -1.1816e+00,\n",
      "          1.8000e+00, -9.8250e-01,  1.4333e+00, -2.9920e-05, -5.5908e-01,\n",
      "         -1.3811e+00,  1.0485e-03, -1.9780e+00,  1.5564e-04,  1.3720e+00,\n",
      "         -1.0006e-02, -7.3661e-02, -2.7940e-01, -2.5237e-01, -1.4921e-03,\n",
      "         -1.1576e+00,  2.3314e+00,  7.2271e-01,  8.1011e-06, -2.3069e-01,\n",
      "          2.2510e+00, -1.1662e-02, -1.7544e+00,  3.2167e-03,  1.4968e+00,\n",
      "          8.1705e-02,  2.2976e-01, -1.5584e+00,  1.6556e+00, -2.3838e+00,\n",
      "         -1.0652e+00, -5.9592e-01,  5.2462e-01, -7.4540e-01, -2.5603e-01,\n",
      "          7.9345e-02,  7.7216e-02,  7.2397e-02,  2.2460e+00,  3.5781e-03,\n",
      "          1.8875e+00, -2.7635e-01, -4.2639e-02, -9.8892e-03, -4.5845e-03,\n",
      "          9.1982e-01,  1.4186e+00, -1.8995e-01,  1.1573e-01,  1.2951e-01,\n",
      "         -1.1289e-04,  1.2320e+00,  4.4674e-03, -2.9280e-01, -7.3407e-01,\n",
      "         -1.8786e-02, -8.0388e-02,  4.2985e-03,  1.8982e+00,  2.8467e-03,\n",
      "         -8.0053e-01, -4.1027e-01, -1.2091e-01, -2.8187e-01,  5.3807e-04,\n",
      "         -4.5844e-02,  4.6866e-01,  1.6601e+00,  3.5879e-03,  1.5547e-06,\n",
      "         -2.9078e-02,  1.1302e+00, -1.2762e-01, -2.2565e+00, -3.3793e-02,\n",
      "         -4.5569e-05,  2.7920e+00, -1.2333e-01, -5.8307e-01, -7.2513e-01,\n",
      "         -4.7518e-01, -1.9479e+00, -1.5045e+00,  7.0520e-03,  8.1668e-01,\n",
      "          6.0961e-01,  7.2466e-04,  1.6600e-03, -1.8877e+00, -9.0438e-02,\n",
      "         -7.5555e-02,  9.5959e-01,  2.3380e-01, -2.2322e+00,  2.8091e-04,\n",
      "         -1.2878e-03, -2.2183e+00, -1.2448e+00, -1.0549e+00, -1.4079e+00,\n",
      "          2.7709e+00,  5.2279e-03,  1.0802e+00, -1.6198e-07,  1.3281e-01,\n",
      "         -3.0514e-01, -7.9592e-01, -2.4979e+00, -8.6330e-02, -1.6396e-02,\n",
      "         -5.2441e-05, -4.9785e-02, -8.6558e-01, -2.5300e+00,  1.9518e+00,\n",
      "          2.1767e+00,  1.4050e-01, -2.6597e-02, -1.3966e-02,  2.1894e+00,\n",
      "          1.9173e+00,  6.2747e-01,  9.5830e-01, -1.2817e+00, -2.1071e+00,\n",
      "          1.0411e-03, -9.4689e-01,  2.1203e-04,  5.3962e-01,  2.1525e+00,\n",
      "          2.0753e+00, -1.6658e+00, -7.2493e-03, -1.1813e+00,  4.0471e-01,\n",
      "          2.0318e-02,  2.6416e-01, -1.6826e+00, -2.7243e-04, -9.3913e-01,\n",
      "         -1.9992e+00, -8.9892e-01, -1.2224e+00,  6.5822e-04,  1.5417e+00,\n",
      "         -1.2114e-03,  3.4063e-01, -9.0878e-04, -9.7787e-01,  1.2108e-03,\n",
      "          9.3352e-02,  2.3570e-02, -1.9609e+00,  9.8145e-01,  2.0511e+00,\n",
      "          2.6938e-02,  3.6093e-02,  1.9189e+00,  1.6863e+00, -1.8634e+00,\n",
      "         -6.6484e-01, -8.9623e-01,  2.1304e+00, -1.6574e-01,  2.0733e-01,\n",
      "          4.7091e-03, -3.0261e-04,  1.1922e-01, -1.0971e-01,  4.2082e-01,\n",
      "          1.7076e-02,  1.2614e-01, -6.2180e-01, -5.3227e-01, -1.2860e-01,\n",
      "          2.3971e-01, -1.2467e+00,  7.0617e-03, -3.6820e-03,  2.7488e-03,\n",
      "          3.1648e-03,  3.1056e-01, -1.6530e-01, -8.0464e-01, -1.5306e-01,\n",
      "          1.7155e+00, -2.3445e-01, -5.1565e-02,  7.0413e-04, -2.4277e-03,\n",
      "         -5.6073e-01, -7.6675e-01,  1.1523e-01, -1.0824e-01,  1.3297e+00,\n",
      "          3.3910e-02, -1.9337e+00, -1.7235e+00, -2.1191e-01, -1.0733e+00,\n",
      "          3.5455e-01,  1.1075e+00, -3.2864e-01, -1.9572e+00, -1.1695e-02,\n",
      "          1.1417e-01, -1.3453e-02,  6.7672e-03, -1.8985e+00,  2.0217e-03,\n",
      "         -1.3575e+00, -7.9730e-02,  5.0792e-04,  1.2400e+00, -9.7234e-01,\n",
      "         -1.7602e+00, -2.5878e-02, -7.5429e-01,  1.3664e-02, -2.4896e-02,\n",
      "          1.1269e+00,  7.8545e-01, -8.1253e-02, -1.9206e+00, -1.8239e-01,\n",
      "         -3.8955e-01,  1.6192e+00,  3.3027e-02,  1.9859e-03,  2.5243e-02,\n",
      "         -5.5193e-02, -5.5494e-03,  1.5712e+00, -1.8195e+00, -1.7138e+00,\n",
      "          4.1074e-02,  7.1689e-03,  6.0312e-01,  5.3472e-01,  8.4051e-03,\n",
      "          3.7656e-02,  5.7096e-02,  1.5000e-02,  5.3974e-02, -5.1153e-02,\n",
      "          1.8660e+00],\n",
      "        [-4.8725e-01,  6.3094e-02, -3.7463e-02, -1.1892e+00, -1.5179e+00,\n",
      "          1.1538e-04, -1.2397e-01,  3.0971e-02,  2.6247e+00,  1.1914e+00,\n",
      "         -1.8149e+00,  9.9063e-01, -1.4452e+00,  3.0168e-05,  5.6370e-01,\n",
      "          1.3925e+00, -1.0572e-03,  1.9943e+00, -1.5693e-04, -1.3834e+00,\n",
      "          1.0089e-02,  7.4270e-02,  2.8172e-01,  2.5446e-01,  1.5044e-03,\n",
      "          1.1672e+00, -2.3507e+00, -7.2869e-01, -8.1681e-06,  2.3260e-01,\n",
      "         -2.2696e+00,  1.1759e-02,  1.7690e+00, -3.2433e-03, -1.5092e+00,\n",
      "         -8.2381e-02, -2.3166e-01,  1.5713e+00, -1.6693e+00,  2.4035e+00,\n",
      "          1.0740e+00,  6.0085e-01, -5.2896e-01,  7.5157e-01,  2.5814e-01,\n",
      "         -8.0002e-02, -7.7855e-02, -7.2996e-02, -2.2646e+00, -3.6077e-03,\n",
      "         -1.9031e+00,  2.7864e-01,  4.2992e-02,  9.9710e-03,  4.6225e-03,\n",
      "         -9.2743e-01, -1.4303e+00,  1.9152e-01, -1.1669e-01, -1.3058e-01,\n",
      "          1.1383e-04, -1.2422e+00, -4.5044e-03,  2.9522e-01,  7.4014e-01,\n",
      "          1.8942e-02,  8.1053e-02, -4.3341e-03, -1.9139e+00, -2.8702e-03,\n",
      "          8.0715e-01,  4.1367e-01,  1.2191e-01,  2.8420e-01, -5.4252e-04,\n",
      "          4.6224e-02, -4.7254e-01, -1.6738e+00, -3.6175e-03, -1.5676e-06,\n",
      "          2.9318e-02, -1.1396e+00,  1.2868e-01,  2.2752e+00,  3.4073e-02,\n",
      "          4.5946e-05, -2.8151e+00,  1.2435e-01,  5.8790e-01,  7.3113e-01,\n",
      "          4.7912e-01,  1.9641e+00,  1.5169e+00, -7.1103e-03, -8.2343e-01,\n",
      "         -6.1465e-01, -7.3065e-04, -1.6738e-03,  1.9033e+00,  9.1186e-02,\n",
      "          7.6180e-02, -9.6753e-01, -2.3573e-01,  2.2507e+00, -2.8323e-04,\n",
      "          1.2984e-03,  2.2366e+00,  1.2551e+00,  1.0636e+00,  1.4195e+00,\n",
      "         -2.7939e+00, -5.2711e-03, -1.0891e+00,  1.6332e-07, -1.3391e-01,\n",
      "          3.0766e-01,  8.0251e-01,  2.5186e+00,  8.7045e-02,  1.6532e-02,\n",
      "          5.2875e-05,  5.0197e-02,  8.7274e-01,  2.5509e+00, -1.9679e+00,\n",
      "         -2.1947e+00, -1.4166e-01,  2.6817e-02,  1.4082e-02, -2.2075e+00,\n",
      "         -1.9332e+00, -6.3267e-01, -9.6623e-01,  1.2923e+00,  2.1246e+00,\n",
      "         -1.0497e-03,  9.5472e-01, -2.1379e-04, -5.4408e-01, -2.1703e+00,\n",
      "         -2.0924e+00,  1.6796e+00,  7.3093e-03,  1.1911e+00, -4.0806e-01,\n",
      "         -2.0486e-02, -2.6635e-01,  1.6966e+00,  2.7469e-04,  9.4690e-01,\n",
      "          2.0157e+00,  9.0636e-01,  1.2326e+00, -6.6367e-04, -1.5545e+00,\n",
      "          1.2214e-03, -3.4345e-01,  9.1630e-04,  9.8596e-01, -1.2208e-03,\n",
      "         -9.4125e-02, -2.3765e-02,  1.9771e+00, -9.8958e-01, -2.0681e+00,\n",
      "         -2.7161e-02, -3.6392e-02, -1.9348e+00, -1.7002e+00,  1.8788e+00,\n",
      "          6.7034e-01,  9.0364e-01, -2.1480e+00,  1.6711e-01, -2.0904e-01,\n",
      "         -4.7481e-03,  3.0511e-04, -1.2020e-01,  1.1062e-01, -4.2430e-01,\n",
      "         -1.7218e-02, -1.2718e-01,  6.2695e-01,  5.3667e-01,  1.2967e-01,\n",
      "         -2.4169e-01,  1.2570e+00, -7.1201e-03,  3.7125e-03, -2.7715e-03,\n",
      "         -3.1910e-03, -3.1313e-01,  1.6667e-01,  8.1130e-01,  1.5432e-01,\n",
      "         -1.7297e+00,  2.3639e-01,  5.1992e-02, -7.0996e-04,  2.4478e-03,\n",
      "          5.6537e-01,  7.7309e-01, -1.1619e-01,  1.0913e-01, -1.3407e+00,\n",
      "         -3.4191e-02,  1.9497e+00,  1.7378e+00,  2.1366e-01,  1.0822e+00,\n",
      "         -3.5748e-01, -1.1166e+00,  3.3136e-01,  1.9734e+00,  1.1791e-02,\n",
      "         -1.1511e-01,  1.3565e-02, -6.8232e-03,  1.9142e+00, -2.0385e-03,\n",
      "          1.3687e+00,  8.0390e-02, -5.1212e-04, -1.2503e+00,  9.8039e-01,\n",
      "          1.7748e+00,  2.6093e-02,  7.6053e-01, -1.3777e-02,  2.5102e-02,\n",
      "         -1.1362e+00, -7.9195e-01,  8.1925e-02,  1.9365e+00,  1.8390e-01,\n",
      "          3.9278e-01, -1.6326e+00, -3.3301e-02, -2.0023e-03, -2.5452e-02,\n",
      "          5.5649e-02,  5.5954e-03, -1.5842e+00,  1.8346e+00,  1.7279e+00,\n",
      "         -4.1414e-02, -7.2282e-03, -6.0811e-01, -5.3914e-01, -8.4746e-03,\n",
      "         -3.7968e-02, -5.7568e-02, -1.5124e-02, -5.4421e-02,  5.1576e-02,\n",
      "         -1.8815e+00],\n",
      "        [ 3.9991e-03, -5.1785e-04,  3.0748e-04,  9.7604e-03,  1.2458e-02,\n",
      "         -9.4701e-07,  1.0175e-03, -2.5419e-04, -2.1542e-02, -9.7786e-03,\n",
      "          1.4896e-02, -8.1306e-03,  1.1861e-02, -2.4760e-07, -4.6266e-03,\n",
      "         -1.1429e-02,  8.6771e-06, -1.6368e-02,  1.2880e-06,  1.1354e-02,\n",
      "         -8.2806e-05, -6.0957e-04, -2.3122e-03, -2.0885e-03, -1.2347e-05,\n",
      "         -9.5796e-03,  1.9293e-02,  5.9807e-03,  6.7040e-08, -1.9091e-03,\n",
      "          1.8628e-02, -9.6509e-05, -1.4519e-02,  2.6619e-05,  1.2386e-02,\n",
      "          6.7615e-04,  1.9014e-03, -1.2896e-02,  1.3701e-02, -1.9727e-02,\n",
      "         -8.8152e-03, -4.9315e-03,  4.3415e-03, -6.1685e-03, -2.1187e-03,\n",
      "          6.5662e-04,  6.3900e-04,  5.9912e-04,  1.8587e-02,  2.9610e-05,\n",
      "          1.5620e-02, -2.2870e-03, -3.5285e-04, -8.1837e-05, -3.7939e-05,\n",
      "          7.6119e-03,  1.1740e-02, -1.5719e-03,  9.5774e-04,  1.0717e-03,\n",
      "         -9.3424e-07,  1.0195e-02,  3.6970e-05, -2.4230e-03, -6.0747e-03,\n",
      "         -1.5546e-04, -6.6524e-04,  3.5572e-05,  1.5708e-02,  2.3558e-05,\n",
      "         -6.6247e-03, -3.3952e-03, -1.0006e-03, -2.3326e-03,  4.4528e-06,\n",
      "         -3.7938e-04,  3.8784e-03,  1.3738e-02,  2.9691e-05,  1.2866e-08,\n",
      "         -2.4063e-04,  9.3530e-03, -1.0561e-03, -1.8673e-02, -2.7966e-04,\n",
      "         -3.7710e-07,  2.3105e-02, -1.0206e-03, -4.8252e-03, -6.0007e-03,\n",
      "         -3.9323e-03, -1.6120e-02, -1.2450e-02,  5.8358e-05,  6.7583e-03,\n",
      "          5.0448e-03,  5.9968e-06,  1.3737e-05, -1.5621e-02, -7.4841e-04,\n",
      "         -6.2525e-04,  7.9410e-03,  1.9348e-03, -1.8473e-02,  2.3247e-06,\n",
      "         -1.0657e-05, -1.8357e-02, -1.0302e-02, -8.7298e-03, -1.1651e-02,\n",
      "          2.2931e-02,  4.3263e-05,  8.9391e-03, -1.3404e-09,  1.0991e-03,\n",
      "         -2.5252e-03, -6.5866e-03, -2.0671e-02, -7.1442e-04, -1.3569e-04,\n",
      "         -4.3397e-07, -4.1199e-04, -7.1630e-03, -2.0937e-02,  1.6152e-02,\n",
      "          1.8013e-02,  1.1627e-03, -2.2010e-04, -1.1557e-04,  1.8118e-02,\n",
      "          1.5867e-02,  5.1926e-03,  7.9303e-03, -1.0607e-02, -1.7437e-02,\n",
      "          8.6156e-06, -7.8359e-03,  1.7547e-06,  4.4655e-03,  1.7813e-02,\n",
      "          1.7174e-02, -1.3786e-02, -5.9991e-05, -9.7760e-03,  3.3491e-03,\n",
      "          1.6814e-04,  2.1861e-03, -1.3925e-02, -2.2545e-06, -7.7717e-03,\n",
      "         -1.6544e-02, -7.4389e-03, -1.0116e-02,  5.4471e-06,  1.2758e-02,\n",
      "         -1.0024e-05,  2.8189e-03, -7.5205e-06, -8.0923e-03,  1.0020e-05,\n",
      "          7.7253e-04,  1.9505e-04, -1.6227e-02,  8.1220e-03,  1.6974e-02,\n",
      "          2.2292e-04,  2.9868e-04,  1.5880e-02,  1.3955e-02, -1.5420e-02,\n",
      "         -5.5018e-03, -7.4167e-03,  1.7630e-02, -1.3715e-03,  1.7157e-03,\n",
      "          3.8970e-05, -2.5042e-06,  9.8657e-04, -9.0791e-04,  3.4824e-03,\n",
      "          1.4131e-04,  1.0439e-03, -5.1457e-03, -4.4047e-03, -1.0643e-03,\n",
      "          1.9837e-03, -1.0317e-02,  5.8438e-05, -3.0470e-05,  2.2747e-05,\n",
      "          2.6190e-05,  2.5700e-03, -1.3680e-03, -6.6588e-03, -1.2666e-03,\n",
      "          1.4196e-02, -1.9402e-03, -4.2672e-04,  5.8270e-06, -2.0090e-05,\n",
      "         -4.6403e-03, -6.3452e-03,  9.5361e-04, -8.9570e-04,  1.1004e-02,\n",
      "          2.8062e-04, -1.6002e-02, -1.4263e-02, -1.7536e-03, -8.8821e-03,\n",
      "          2.9341e-03,  9.1647e-03, -2.7196e-03, -1.6197e-02, -9.6777e-05,\n",
      "          9.4480e-04, -1.1133e-04,  5.6002e-05, -1.5711e-02,  1.6731e-05,\n",
      "         -1.1234e-02, -6.5980e-04,  4.2032e-06,  1.0262e-02, -8.0466e-03,\n",
      "         -1.4567e-02, -2.1416e-04, -6.2420e-03,  1.1308e-04, -2.0602e-04,\n",
      "          9.3254e-03,  6.4999e-03, -6.7240e-04, -1.5894e-02, -1.5094e-03,\n",
      "         -3.2237e-03,  1.3400e-02,  2.7332e-04,  1.6434e-05,  2.0890e-04,\n",
      "         -4.5674e-04, -4.5924e-05,  1.3002e-02, -1.5057e-02, -1.4182e-02,\n",
      "          3.3990e-04,  5.9325e-05,  4.9911e-03,  4.4250e-03,  6.9556e-05,\n",
      "          3.1162e-04,  4.7249e-04,  1.2413e-04,  4.4666e-04, -4.2331e-04,\n",
      "          1.5442e-02]])\n",
      "by.grad: tensor([6.5193e-09, 6.5193e-09, 6.5193e-09])\n",
      "predicted : tensor([[ 5.3339,  5.3339,  5.3339],\n",
      "        [-1.4931, -1.4931, -1.4931],\n",
      "        [ 0.5394,  0.5394,  0.5394]], grad_fn=<AddBackward0>)\n",
      "Output (softmax probabilities): tensor([[0.9907, 0.9907, 0.9907],\n",
      "        [0.0011, 0.0011, 0.0011],\n",
      "        [0.0082, 0.0082, 0.0082]], grad_fn=<SoftmaxBackward0>)\n",
      "target : tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 3\n",
    "\n",
    "# Initialize weights and biases for forward LSTM\n",
    "Wi_f = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Ui_f = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bi_f = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wf_f = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uf_f = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bf_f = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wo_f = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uo_f = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bo_f = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wc_f = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uc_f = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bc_f = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "# Initialize weights and biases for backward LSTM\n",
    "Wi_b = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Ui_b = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bi_b = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wf_b = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uf_b = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bf_b = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wo_b = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uo_b = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bo_b = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "Wc_b = torch.randn(hidden_size, input_size, requires_grad=True)\n",
    "Uc_b = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "bc_b = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "# Initialize weights for the output layer\n",
    "Why = torch.randn(output_size, 2 * hidden_size, requires_grad=True)\n",
    "by = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Sample input (sequence length = 3, batch size = 1, input size = 10)\n",
    "inputs = torch.randn(3, input_size)\n",
    "\n",
    "# Define targets for classification\n",
    "targets = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "# Initialize hidden state and cell state for forward LSTM\n",
    "h_f = torch.zeros(hidden_size, 1, requires_grad=True)\n",
    "c_f = torch.zeros(hidden_size, 1, requires_grad=True)\n",
    "\n",
    "# Initialize hidden state and cell state for backward LSTM\n",
    "h_b = torch.zeros(hidden_size, 1, requires_grad=True)\n",
    "c_b = torch.zeros(hidden_size, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass for forward LSTM\n",
    "outputs_f = []\n",
    "for i in range(inputs.size(0)):\n",
    "    input_t = inputs[i].unsqueeze(1)\n",
    "    print(\"inputt\", input_t)\n",
    "    f = torch.sigmoid(torch.matmul(Wf_f, input_t) + torch.matmul(Uf_f, h_f) + bf_f.unsqueeze(1))\n",
    "    i_gate = torch.sigmoid(torch.matmul(Wi_f, input_t) + torch.matmul(Ui_f, h_f) + bi_f.unsqueeze(1))\n",
    "    o = torch.sigmoid(torch.matmul(Wo_f, input_t) + torch.matmul(Uo_f, h_f) + bo_f.unsqueeze(1))\n",
    "    c_hat = torch.tanh(torch.matmul(Wc_f, input_t) + torch.matmul(Uc_f, h_f) + bc_f.unsqueeze(1))\n",
    "    c_f = f * c_f + i_gate * c_hat\n",
    "    h_f = o * torch.tanh(c_f)\n",
    "    outputs_f.append(h_f)\n",
    "\n",
    "# Forward pass for backward LSTM\n",
    "outputs_b = []\n",
    "for i in reversed(range(inputs.size(0))):\n",
    "    input_t = inputs[i].unsqueeze(1)\n",
    "    print(\"rinputt\", input_t)\n",
    "    f = torch.sigmoid(torch.matmul(Wf_b, input_t) + torch.matmul(Uf_b, h_b) + bf_b.unsqueeze(1))\n",
    "    i_gate = torch.sigmoid(torch.matmul(Wi_b, input_t) + torch.matmul(Ui_b, h_b) + bi_b.unsqueeze(1))\n",
    "    o = torch.sigmoid(torch.matmul(Wo_b, input_t) + torch.matmul(Uo_b, h_b) + bo_b.unsqueeze(1))\n",
    "    c_hat = torch.tanh(torch.matmul(Wc_b, input_t) + torch.matmul(Uc_b, h_b) + bc_b.unsqueeze(1))\n",
    "    c_b = f * c_b + i_gate * c_hat\n",
    "    h_b = o * torch.tanh(c_b)\n",
    "    outputs_b.insert(0, h_b)\n",
    "\n",
    "# Concatenate the outputs from forward and backward LSTMs\n",
    "outputs = [torch.cat((h_f, h_b), dim=0) for h_f, h_b in zip(outputs_f, outputs_b)]\n",
    "outputs = torch.stack(outputs, dim=0).squeeze(1)\n",
    "\n",
    "# Compute output for the last time step\n",
    "output = torch.matmul(Why, outputs[-1]) + by\n",
    "\n",
    "# Compute loss (negative log likelihood loss)\n",
    "log_probs = torch.nn.functional.log_softmax(output, dim=0)\n",
    "loss = -log_probs[targets].sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Print loss and gradients\n",
    "print('Loss:', loss.item())\n",
    "print('Gradients:')\n",
    "print('Wi_f.grad:', Wi_f.grad)\n",
    "print('Ui_f.grad:', Ui_f.grad)\n",
    "print('bi_f.grad:', bi_f.grad)\n",
    "print('Wf_f.grad:', Wf_f.grad)\n",
    "print('Uf_f.grad:', Uf_f.grad)\n",
    "print('bf_f.grad:', bf_f.grad)\n",
    "print('Wo_f.grad:', Wo_f.grad)\n",
    "print('Uo_f.grad:', Uo_f.grad)\n",
    "print('bo_f.grad:', bo_f.grad)\n",
    "print('Wc_f.grad:', Wc_f.grad)\n",
    "print('Uc_f.grad:', Uc_f.grad)\n",
    "print('bc_f.grad:', bc_f.grad)\n",
    "print('Wi_b.grad:', Wi_b.grad)\n",
    "print('Ui_b.grad:', Ui_b.grad)\n",
    "print('bi_b.grad:', bi_b.grad)\n",
    "print('Wf_b.grad:', Wf_b.grad)\n",
    "print('Uf_b.grad:', Uf_b.grad)\n",
    "print('bf_b.grad:', bf_b.grad)\n",
    "print('Wo_b.grad:', Wo_b.grad)\n",
    "print('Uo_b.grad:', Uo_b.grad)\n",
    "print('bo_b.grad:', bo_b.grad)\n",
    "print('Wc_b.grad:', Wc_b.grad)\n",
    "print('Uc_b.grad:', Uc_b.grad)\n",
    "print('bc_b.grad:', bc_b.grad)\n",
    "print('Why.grad:', Why.grad)\n",
    "print('by.grad:', by.grad)\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    Wi_f -= learning_rate * Wi_f.grad\n",
    "    Ui_f -= learning_rate * Ui_f.grad\n",
    "    bi_f -= learning_rate * bi_f.grad\n",
    "    Wf_f -= learning_rate * Wf_f.grad\n",
    "    Uf_f -= learning_rate * Uf_f.grad\n",
    "    bf_f -= learning_rate * bf_f.grad\n",
    "    Wo_f -= learning_rate * Wo_f.grad\n",
    "    Uo_f -= learning_rate * Uo_f.grad\n",
    "    bo_f -= learning_rate * bo_f.grad\n",
    "    Wc_f -= learning_rate * Wc_f.grad\n",
    "    Uc_f -= learning_rate * Uc_f.grad\n",
    "    bc_f -= learning_rate * bc_f.grad\n",
    "    Wi_b -= learning_rate * Wi_b.grad\n",
    "    Ui_b -= learning_rate * Ui_b.grad\n",
    "    bi_b -= learning_rate * bi_b.grad\n",
    "    Wf_b -= learning_rate * Wf_b.grad\n",
    "    Uf_b -= learning_rate * Uf_b.grad\n",
    "    bf_b -= learning_rate * bf_b.grad\n",
    "    Wo_b -= learning_rate * Wo_b.grad\n",
    "    Uo_b -= learning_rate * Uo_b.grad\n",
    "    bo_b -= learning_rate * bo_b.grad\n",
    "    Wc_b -= learning_rate * Wc_b.grad\n",
    "    Uc_b -= learning_rate * Uc_b.grad\n",
    "    bc_b -= learning_rate * bc_b.grad\n",
    "    Why -= learning_rate * Why.grad\n",
    "    by -= learning_rate * by.grad\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    Wi_f.grad.zero_()\n",
    "    Ui_f.grad.zero_()\n",
    "    bi_f.grad.zero_()\n",
    "    Wf_f.grad.zero_()\n",
    "    Uf_f.grad.zero_()\n",
    "    bf_f.grad.zero_()\n",
    "    Wo_f.grad.zero_()\n",
    "    Uo_f.grad.zero_()\n",
    "    bo_f.grad.zero_()\n",
    "    Wc_f.grad.zero_()\n",
    "    Uc_f.grad.zero_()\n",
    "    bc_f.grad.zero_()\n",
    "    Wi_b.grad.zero_()\n",
    "    Ui_b.grad.zero_()\n",
    "    bi_b.grad.zero_()\n",
    "    Wf_b.grad.zero_()\n",
    "    Uf_b.grad.zero_()\n",
    "    bf_b.grad.zero_()\n",
    "    Wo_b.grad.zero_()\n",
    "    Uo_b.grad.zero_()\n",
    "    bo_b.grad.zero_()\n",
    "    Wc_b.grad.zero_()\n",
    "    Uc_b.grad.zero_()\n",
    "    bc_b.grad.zero_()\n",
    "    Why.grad.zero_()\n",
    "    by.grad.zero_()\n",
    "\n",
    "print(\"predicted :\", output)\n",
    "softmax = torch.nn.functional.softmax(output, dim=0)\n",
    "print('Output (softmax probabilities):', softmax)\n",
    "print(\"target :\", targets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
